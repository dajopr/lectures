{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ0PzRZTnW23sucpxCFKQK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajopr/lectures/blob/main/image_processing/lecture_03_keypoint_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenCV Exercise Sheet: Feature extraction - Corners and blobs\n",
        "\n",
        "Prerequisites: Basic knowledge of Python, NumPy, OpenCV (reading/displaying/converting images),\n",
        "               Spatial Filtering concepts, Edge Detection concepts, Matplotlib.\n",
        "Goal: This exercise sheet focuses on detecting key features like corners and blobs\n",
        "      in images using OpenCV, understanding detector parameters, and comparing methods."
      ],
      "metadata": {
        "id": "8qKp1-Pd-TGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os # Used for checking file paths\n",
        "\n",
        "# %% Helper function to display images (Adapted from lecture_02_spatial_filtering.py)\n",
        "def show_images(images, titles, rows, cols, figsize=(15, 10), main_title=\"Image Comparison\"):\n",
        "    \"\"\"Helper function to display multiple images using Matplotlib\"\"\"\n",
        "    if len(images) != len(titles):\n",
        "        print(\"Warning: Number of images and titles do not match.\")\n",
        "        # Attempt to proceed with the minimum of the two counts\n",
        "        min_count = min(len(images), len(titles))\n",
        "        images = images[:min_count]\n",
        "        titles = titles[:min_count]\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "    fig.suptitle(main_title, fontsize=16)\n",
        "\n",
        "    # Handle case where subplot returns a single Axes object or 1D array\n",
        "    if rows * cols == 1:\n",
        "        axes = np.array([axes])\n",
        "    axes = axes.ravel() # Flatten the axes array\n",
        "\n",
        "    plot_index = 0\n",
        "    for i, img in enumerate(images):\n",
        "        if plot_index < len(axes): # Ensure we don't exceed the number of axes\n",
        "            current_ax = axes[plot_index]\n",
        "            if img is not None:\n",
        "                # Check if image is color or grayscale\n",
        "                if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):\n",
        "                    # Display grayscale image\n",
        "                    current_ax.imshow(img, cmap='gray')\n",
        "                elif len(img.shape) == 3 and img.shape[2] == 3:\n",
        "                    # Assume BGR format from OpenCV, convert to RGB for Matplotlib\n",
        "                    current_ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "                elif len(img.shape) == 3 and img.shape[2] == 4:\n",
        "                     # Assume BGRA format from OpenCV, convert to RGBA for Matplotlib\n",
        "                    current_ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA))\n",
        "                else:\n",
        "                    current_ax.set_title(f'{titles[i]} (Unsupported Format)')\n",
        "                    print(f\"Warning: Image '{titles[i]}' has an unsupported shape: {img.shape}\")\n",
        "\n",
        "                if len(img.shape) != 3 or img.shape[2] != 4: # Don't overwrite title if unsupported format message was set\n",
        "                     current_ax.set_title(titles[i])\n",
        "\n",
        "            else:\n",
        "                current_ax.set_title(f'{titles[i]} (None)') # Handle None images\n",
        "            current_ax.axis('off') # Hide axes ticks\n",
        "            plot_index += 1\n",
        "        else:\n",
        "            print(f\"Warning: More images provided than subplot slots ({rows*cols}). Skipping '{titles[i]}'.\")\n",
        "\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(plot_index, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "f2IRHX3q-VLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Exercise 1: Basic Harris Corner Detection\n",
        "\n",
        "Objective: Understand and apply the Harris corner detector using OpenCV.\n",
        "            Visualize the detected corners on an image.\n",
        "\n",
        "Instructions:\n",
        "1. Load the chessboard image (`IMG_PATH_CHESS`) or shapes image (`IMG_PATH_SHAPES`).\n",
        "    Handle potential loading errors.\n",
        "2. Convert the loaded image to grayscale.\n",
        "3. Apply the `cv2.cornerHarris()` function to the grayscale image.\n",
        "    - Use parameters: `blockSize=2`, `ksize=3`, `k=0.04`.\n",
        "    - Remember `cv2.cornerHarris` requires a float32 input image.\n",
        "4. The result `dst` from `cornerHarris` is a float image with corner response scores.\n",
        "   Threshold this result to identify strong corners. A common approach is:\n",
        "   `threshold = 0.01 * dst.max()`\n",
        "   Corners are pixels where `dst > threshold`.\n",
        "5. Create a copy of the original *color* image to draw on.\n",
        "6. Mark the detected corners on the color image copy. You can do this by:\n",
        "   a) Setting the pixel color directly: `img_draw[dst > threshold] = [255, 0, 0]` (Marks in Blue for BGR)\n",
        "   b) Finding the coordinates `(x, y)` where `dst > threshold` (e.g., using `np.where`)\n",
        "   and then drawing circles at these coordinates using `cv2.circle()`. Method (b) is often clearer.\n",
        "7. Display the original image and the image with detected corners using the `show_images` function."
      ],
      "metadata": {
        "id": "skMlx5Kr-c34"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOYCloXH-sM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion Questions (Exercise 1):\n",
        "1. What happens visually if you increase the `k` parameter in `cv2.cornerHarris()` (e.g., to 0.1)? What if you decrease it (e.g., to 0.01)?\n",
        "2. What is the effect of changing the `blockSize` parameter? Try `blockSize=5`.\n",
        "3. Looking at the `chessboard.png`, does Harris find corners mostly where you expect them? Are there any surprises?"
      ],
      "metadata": {
        "id": "BLHSQftC-s85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Shi-Tomasi (\"Good Features to Track\") vs. Harris\n",
        "Objective: Compare the Harris corner detector with the Shi-Tomasi detector (`cv2.goodFeaturesToTrack`).\n",
        "           Understand the parameters and output differences.\n",
        "Instructions:\n",
        "1. Load the same image used in Exercise 1 (`IMG_PATH_CHESS` or `IMG_PATH_SHAPES`).\n",
        "2. Convert it to grayscale.\n",
        "3. Detect corners using `cv2.cornerHarris()` as you did in Exercise 1. Draw these corners\n",
        "   on a copy of the original color image (e.g., using blue circles).\n",
        "4. Detect corners using `cv2.goodFeaturesToTrack()` on the grayscale image.\n",
        "   - Use parameters like `maxCorners=100`, `qualityLevel=0.01`, `minDistance=10`.\n",
        "   - This function directly returns the corner coordinates. Remember to convert them to integers\n",
        "     (e.g., using `np.intp()` or `np.int0()`) before drawing.\n",
        "5. Draw the Shi-Tomasi corners on a *different* copy of the original color image (e.g., using green circles).\n",
        "6. Display the Harris results and Shi-Tomasi results side-by-side using `show_images`."
      ],
      "metadata": {
        "id": "8A2jhttI-6tL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CldMdfUp--aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion Questions (Exercise 2):\n",
        "1. Compare the *distribution* of corners found by Harris (after thresholding) and Shi-Tomasi. How does `minDistance` in Shi-Tomasi affect this?\n",
        "2. How does changing the `qualityLevel` in Shi-Tomasi affect the *number* of corners found?\n",
        "3. Based on the visual results, which detector seems to give more evenly spaced, well-defined corners, potentially better suited for tracking applications? Why?"
      ],
      "metadata": {
        "id": "MxsSWUWp-_lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Basic Blob Detection with `SimpleBlobDetector`\n",
        "Objective: Learn to use OpenCV's `SimpleBlobDetector` with its default settings\n",
        "           to find blobs (regions of similar intensity) in an image.\n",
        "Instructions:\n",
        "1. Load the `dots.png` image (`IMG_PATH_DOTS`). It's often best to load it directly as grayscale\n",
        "   using `cv2.imread(path, cv2.IMREAD_GRAYSCALE)`. Handle loading errors.\n",
        "2. Create an instance of the `cv2.SimpleBlobDetector` using the default parameters:\n",
        "   `params = cv2.SimpleBlobDetector_Params()`\n",
        "   `detector = cv2.SimpleBlobDetector_create(params)`\n",
        "   *(Note: For older OpenCV versions < 3, it might be `detector = cv2.SimpleBlobDetector(params)`)*\n",
        "3. Detect blobs in the grayscale image using `keypoints = detector.detect(image)`.\n",
        "4. The result `keypoints` is a list of `cv2.KeyPoint` objects.\n",
        "5. Use `cv2.drawKeypoints()` to draw the detected blobs onto the image.\n",
        "   - Use the flag `cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS` to draw circles\n",
        "     representing the size and location of the blobs.\n",
        "   - Example: `img_with_keypoints = cv2.drawKeypoints(image, keypoints, np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)` (Draws\n",
        "6. Display the original grayscale image and the image with detected blobs using `show_images`."
      ],
      "metadata": {
        "id": "a3dUO3vz_LJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxlpHp9D_OFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion Questions (Exercise 3):\n",
        "1. Did the default detector find most/all of the dots? Did it find anything unexpected?\n",
        "2. What information does a `cv2.KeyPoint` object contain? (Hint: try `print(keypoints[0].pt, keypoints[0].size)` if `keypoints` is not empty).\n",
        "3. The default detector looks for dark blobs on a light background if you feed it a standard grayscale image. How could you detect light blobs on a dark background?"
      ],
      "metadata": {
        "id": "BOHtqVdY_SM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Filtering Blobs by Properties\n",
        "Objective: Configure `SimpleBlobDetector` parameters to filter detected blobs based on\n",
        "           properties like Area and Circularity.\n",
        "Instructions:\n",
        "1. Load the `various_blobs.png` image (`IMG_PATH_BLOBS`) as grayscale. This image should\n",
        "   contain blobs of different sizes and shapes. Handle loading errors.\n",
        "2. Create a `cv2.SimpleBlobDetector_Params()` object.\n",
        "3. Modify the parameters object to filter blobs:\n",
        "   - **Filter by Area:**\n",
        "     - `params.filterByArea = True`\n",
        "     - `params.minArea = 100` (Example: Adjust based on your image)\n",
        "     - `params.maxArea = 2000` (Example: Adjust based on your image)\n",
        "   - **Filter by Circularity:** (Circularity = 1 for a perfect circle)\n",
        "     - `params.filterByCircularity = True`\n",
        "     - `params.minCircularity = 0.7` (Example: Find blobs that are mostly round)\n",
        "     - `params.maxCircularity = 1.0` (Usually okay unless excluding circles)\n",
        "   - *Experiment with these values to see their effect.*\n",
        "4. Create the detector using these modified parameters: `detector = cv2.SimpleBlobDetector_create(params)`.\n",
        "5. Detect blobs in the image: `keypoints = detector.detect(image)`.\n",
        "6. Draw the detected (filtered) blobs onto the image using `cv2.drawKeypoints` (like in Ex 3).\n",
        "7. Display the original grayscale image and the image with filtered blobs using `show_images`."
      ],
      "metadata": {
        "id": "BxXR1_Q3_UKt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DY7UlKx_YC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion Questions (Exercise 4):\n",
        "1. How did setting `minArea` and `maxArea` change which blobs were detected compared to Exercise 3 (or default parameters)?\n",
        "2. What was the effect of adding the `minCircularity` filter? Which shapes were excluded?\n",
        "3. Why is it often necessary to tune multiple filter parameters together (e.g., Area *and* Circularity) to isolate specific types of blobs effectively?"
      ],
      "metadata": {
        "id": "LcFvng1o_YY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Exercise: Sub-Pixel Corner Refinement\n",
        "Objective: Improve the accuracy of corner locations detected by Shi-Tomasi\n",
        "           using `cv2.cornerSubPix` to achieve sub-pixel precision.\n",
        "Instructions:\n",
        "1. Load the `chessboard.png` image (`IMG_PATH_CHESS`) and convert to grayscale.\n",
        "2. Detect initial corner estimates using `cv2.goodFeaturesToTrack`. Store these corners\n",
        "   (they should be float32, which is the default output).\n",
        "3. Define the termination criteria for the sub-pixel refinement process. This tells the\n",
        "   algorithm when to stop iterating. A common criteria is:\n",
        "   `criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)`\n",
        "   (Stop after 30 iterations or when accuracy `EPS`=0.001 is reached).\n",
        "4. Use `cv2.cornerSubPix()` to refine the initial corner locations.\n",
        "   - Inputs needed: grayscale image, initial corners (float32), `winSize` (search window size, e.g., `(11, 11)`),\n",
        "     `zeroZone` (usually `(-1, -1)` to ignore), and the `criteria`.\n",
        "   - `corners_refined = cv2.cornerSubPix(gray, corners_initial, winSize=(11, 11), zeroZone=(-1, -1), criteria=criteria)`\n",
        "5. Create a copy of the original color image to draw on.\n",
        "6. Draw both the initial corners (converted to *integers* for drawing, e.g., green circles)\n",
        "   and the refined corners (using their *float* coordinates rounded/cast to int for drawing, e.g., smaller red circles or crosses).\n",
        "7. Display the result. Use `plt.xlim()` and `plt.ylim()` on the Matplotlib plot to zoom\n",
        "   in on a specific corner region to visually inspect the difference between initial\n",
        "   and refined locations."
      ],
      "metadata": {
        "id": "ve8Zdd4i_d82"
      }
    }
  ]
}