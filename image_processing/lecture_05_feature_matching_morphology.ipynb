{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajopr/lectures/blob/main/image_processing/lecture_05_feature_matching_morphology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455fc909",
      "metadata": {
        "id": "455fc909"
      },
      "source": [
        "# Exercise Sheet: Local Feature Detection, Description, Matching & Homography\n",
        "\n",
        "Prerequisites: Basic Python, NumPy, OpenCV (image loading, display, color conversion, drawing functions), Matplotlib. Familiarity with basic image processing concepts.\n",
        "\n",
        "Goal: To understand, implement, and evaluate methods for detecting and describing local image features (e.g., SIFT, ORB), matching these features between images robustly, and computing homography transformations for applications like image alignment and perspective analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd94f2f",
      "metadata": {
        "id": "4bd94f2f"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def show_images(\n",
        "    images, titles, rows, cols, figsize=(15, 10), main_title=\"Image Comparison\"\n",
        "):\n",
        "    \"\"\"Helper function to display multiple images using Matplotlib\"\"\"\n",
        "    if not images:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "    if len(images) != len(titles):\n",
        "        print(\"Warning: Number of images and titles do not match.\")\n",
        "        min_count = min(len(images), len(titles))\n",
        "        images = images[:min_count]\n",
        "        titles = titles[:min_count]\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "    fig.suptitle(main_title, fontsize=16)\n",
        "\n",
        "    if rows * cols == 1:  # Handle single subplot case\n",
        "        axes = np.array([axes])  # Make it iterable like multiple subplots\n",
        "    axes = axes.ravel()  # Flatten the axes array for easy iteration\n",
        "\n",
        "    plot_index = 0\n",
        "    for i, img in enumerate(images):\n",
        "        if plot_index < len(axes):\n",
        "            current_ax = axes[plot_index]\n",
        "            if img is not None:\n",
        "                # Convert BGR (OpenCV default) to RGB for Matplotlib display\n",
        "                if len(img.shape) == 2 or (\n",
        "                    len(img.shape) == 3 and img.shape[2] == 1\n",
        "                ):  # Grayscale\n",
        "                    current_ax.imshow(img, cmap=\"gray\")\n",
        "                    current_ax.set_title(titles[i])\n",
        "                elif len(img.shape) == 3 and img.shape[2] == 3:  # Color (BGR)\n",
        "                    current_ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "                    current_ax.set_title(titles[i])\n",
        "                elif (\n",
        "                    len(img.shape) == 3 and img.shape[2] == 4\n",
        "                ):  # Color with Alpha (BGRA)\n",
        "                    current_ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA))\n",
        "                    current_ax.set_title(titles[i])\n",
        "                else:\n",
        "                    current_ax.text(\n",
        "                        0.5,\n",
        "                        0.5,\n",
        "                        \"Unsupported Format\",\n",
        "                        horizontalalignment=\"center\",\n",
        "                        verticalalignment=\"center\",\n",
        "                        transform=current_ax.transAxes,\n",
        "                    )\n",
        "                    current_ax.set_title(f\"{titles[i]} (Unsupported Format)\")\n",
        "            else:\n",
        "                current_ax.text(\n",
        "                    0.5,\n",
        "                    0.5,\n",
        "                    \"Image is None\",\n",
        "                    horizontalalignment=\"center\",\n",
        "                    verticalalignment=\"center\",\n",
        "                    transform=current_ax.transAxes,\n",
        "                )\n",
        "                current_ax.set_title(f\"{titles[i]} (None)\")\n",
        "            current_ax.axis(\"off\")\n",
        "            plot_index += 1\n",
        "        else:\n",
        "            print(\n",
        "                f\"Warning: More images provided than subplot slots ({rows * cols}). Skipping '{titles[i]}'.\"\n",
        "            )\n",
        "\n",
        "    # Turn off any remaining unused subplots\n",
        "    for j in range(plot_index, len(axes)):\n",
        "        axes[j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for suptitle\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285373e9",
      "metadata": {
        "id": "285373e9"
      },
      "source": [
        "# Exercise 1: Introduction to SIFT (Scale-Invariant Feature Transform)\n",
        "\n",
        "**Objective:** To understand and apply the SIFT (Scale-Invariant Feature Transform) algorithm for keypoint detection and descriptor computation using OpenCV. This exercise will cover initializing the SIFT detector, finding keypoints, computing their descriptors, and visualizing the results.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Load and Prepare Image:**\n",
        "    * Choose one of your test images.\n",
        "    * In your Python script, load it as a color image (e.g., using `cv2.imread()`) and assign it to a variable named `image_color`. You can use `sunflowers.jpg` or any image of your choosing.\n",
        "    * Create a grayscale version of this image (e.g., using `cv2.cvtColor()`) and assign it to a variable named `image_gray`.\n",
        "    * It's good practice to check if the image was loaded successfully.\n",
        "\n",
        "2.  **Initialize SIFT Detector:**\n",
        "    * Create a SIFT detector object using the appropriate OpenCV function (e.g., `cv2.SIFT_create()`).\n",
        "    * Assign this detector object to a variable named `sift`.\n",
        "\n",
        "3.  **Detect Keypoints:**\n",
        "    * Using the `sift` object's detection method (e.g., `sift.detect()`) and the `image_gray`, detect keypoints.\n",
        "    * Assign the resulting list of keypoints to a variable named `keypoints_sift`.\n",
        "    * You can print the number of detected keypoints using `len(keypoints_sift)`.\n",
        "\n",
        "4.  **Examine Keypoint and Descriptor Details:**\n",
        "    * Print the total number of keypoints found by SIFT.\n",
        "    * Print the shape of a `keypoint_sift`.\n",
        "    * *(Optional)* To understand what a keypoint object contains, access an individual keypoint from the `keypoints_sift` list (e.g., `keypoints_sift[0]`) and inspect its attributes like `pt` (coordinates), `size`, `angle`, `response`, and `octave`.\n",
        "\n",
        "5.  **Visualize Keypoints:**\n",
        "    * Draw the detected `keypoints_sift` on the original `image_color`. OpenCV provides a function for this (e.g., `cv2.drawKeypoints()`).\n",
        "    * To see the scale and orientation of keypoints, use a flag like `cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS` when drawing.\n",
        "    * Display the resulting image (with keypoints drawn on it) using a library like Matplotlib (e.g., using `plt.imshow()`). Remember to handle color channel conversion if necessary (OpenCV uses BGR by default, Matplotlib uses RGB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f9142e",
      "metadata": {
        "id": "c8f9142e"
      },
      "outputs": [],
      "source": [
        "def print_sift_keypoint_details(keypoint):\n",
        "    \"\"\"\n",
        "    Prints the relevant fields of a SIFT keypoint descriptor.\n",
        "\n",
        "    Args:\n",
        "        keypoint (cv2.KeyPoint): A single SIFT keypoint object.\n",
        "    \"\"\"\n",
        "    if not isinstance(keypoint, cv2.KeyPoint):\n",
        "        print(\"Invalid input: Expected a cv2.KeyPoint object.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Keypoint Details:\")\n",
        "    print(f\"  - Location (x, y): {keypoint.pt}\")\n",
        "    print(f\"  - Size: {keypoint.size}\")\n",
        "    print(f\"  - Angle: {keypoint.angle}\")\n",
        "    print(f\"  - Response: {keypoint.response}\")\n",
        "    print(f\"  - Octave: {keypoint.octave}\")\n",
        "    print(f\"  - Class ID: {keypoint.class_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634a912b",
      "metadata": {
        "id": "634a912b"
      },
      "outputs": [],
      "source": [
        "image_path = \"IMAGE.png\"  # Change this to selected image\n",
        "\n",
        "image_color = cv2.imread(image_path, cv2.IMREAD_COLOR_RGB)\n",
        "image_gray = cv2.cvtColor(image_color, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# 2. Initialize SIFT detector\n",
        "sift = None  # Change this\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "\n",
        "# 3. Detect keypoints\n",
        "keypoints_sift = None  # Change this\n",
        "\n",
        "# 4. Print number of keypoints and shape of a keypoint object\n",
        "num_keypoints = None  # Change this\n",
        "first_keypoint = None  # Change this\n",
        "\n",
        "print(f\"Number of keypoints: {num_keypoints}\")\n",
        "print_sift_keypoint_details(first_keypoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdaa6e0f",
      "metadata": {
        "id": "fdaa6e0f"
      },
      "source": [
        "# Exercise 2: Comparing Keypoint Detectors (SIFT, ORB, AKAZE)\n",
        "\n",
        "**Objective:** To apply and compare SIFT, ORB, and AKAZE feature detection algorithms on various images, observing their characteristics, the number of keypoints detected, their distribution, and their suitability for different image types.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* Python environment with OpenCV and Matplotlib.\n",
        "* **OpenCV Installation:** Ensure you have the necessary OpenCV packages. SIFT is included in the `opencv-contrib-python` package. If you don't have it, you might need to install or upgrade:\n",
        "    ```bash\n",
        "    pip install opencv-python opencv-contrib-python matplotlib\n",
        "    ```\n",
        "* **Test Images:** Download the test images (or select your own). Consider images with:\n",
        "    1.  Good, varied textures and distinct features (e.g., `landscape_with_buildings.jpg` or `sunflowers.jpg`).\n",
        "    2.  Significant scale changes (e.g., `taj_mahal_far.jpg`, `taj_mahal_close.jpg`).\n",
        "    3.  Rotation (e.g., `book_frontal.png`, `book_rotated.png`).\n",
        "    4.  Perspective distortion (e.g., `sticker_frontal.jpg`, `sticker_angle.jpg`).\n",
        "    5.  Blurred (e.g.`sticker_blurred.jpg` )\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Load and Prepare Image:**\n",
        "    * Choose one of your test images.\n",
        "    * Load it as a color image and assign it to a variable named `image_color`.\n",
        "    * Create a grayscale version of this image and assign it to a variable named `image_gray`.\n",
        "\n",
        "2.  **Initialize Detectors:**\n",
        "    * Create a default SIFT detector object and assign it to a variable named `sift`.\n",
        "    * Create a default ORB detector object and assign it to a variable named `orb`.\n",
        "        * *(Optional: You can experiment with `nfeatures` for ORB, e.g., `cv2.ORB_create(nfeatures=1000)` after trying the default).*\n",
        "    * Create a default AKAZE detector object and assign it to a variable named `akaze`.\n",
        "\n",
        "3.  **Detect Keypoints (Features):**\n",
        "    * Using the `sift` object and `image_gray`, detect keypoints and assign the resulting list of keypoints to a variable named `keypoints_sift`.\n",
        "    * Using the `orb` object and `image_gray`, detect keypoints and assign them to a variable named `keypoints_orb`.\n",
        "    * Using the `akaze` object and `image_gray`, detect keypoints and assign them to a variable named `keypoints_akaze`.\n",
        "\n",
        "4.  **Examine Keypoint Counts:**\n",
        "    * Print the number of keypoints found by each detector. For example:\n",
        "        `print(f\"SIFT detected {len(keypoints_sift)} keypoints.\")`\n",
        "        `print(f\"ORB detected {len(keypoints_orb)} keypoints.\")`\n",
        "        `print(f\"AKAZE detected {len(keypoints_akaze)} keypoints.\")`\n",
        "\n",
        "5.  **Visualize Keypoints:**\n",
        "    * The provided code (in the Jupyter Notebook, to be supplied by your instructor) will take `image_color`, `keypoints_sift`, `keypoints_orb`, and `keypoints_akaze` to draw these keypoints on separate copies of the image and display them in a single plot for comparison. Ensure your variables are named correctly for the provided display code to work.\n",
        "\n",
        "6.  **Experiment:**\n",
        "    * After observing the results on your first selected image, run the same detection steps (1-4) on a different image that has distinct characteristics (e.g., more/less texture, significant rotation or scale difference compared to an imaginary \"original\"). This will help inform your answers to the discussion questions.\n",
        "\n",
        "7.  **Answer the Discussion Questions.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222f977b",
      "metadata": {
        "id": "222f977b"
      },
      "outputs": [],
      "source": [
        "image_path = \"images/IMAGE.jpg\"  # <<< CHANGE THIS TO YOUR IMAGE PATH\n",
        "\n",
        "image_color = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "image_color = cv2.resize(image_color, None, fx=0.5, fy=0.5)\n",
        "if image_color is None:\n",
        "    raise FileNotFoundError(f\"Image not found or unable to read: {image_path}\")\n",
        "image_gray = cv2.cvtColor(image_color, cv2.COLOR_BGR2GRAY)\n",
        "# --- Student Code Starts Here ---\n",
        "\n",
        "# Create instances of SIFT, ORB (nfeatures=500) and AKAZE detectors\n",
        "sift = None\n",
        "orb = None\n",
        "akaze = None\n",
        "\n",
        "\n",
        "# Detect keypoints using the detectors\n",
        "keypoints_sift = []\n",
        "keypoints_orb = []\n",
        "keypoints_akaze = []\n",
        "\n",
        "\n",
        "# --- Student Code Ends Here ---\n",
        "img_display_sift = image_color.copy()\n",
        "img_display_orb = image_color.copy()\n",
        "img_display_akaze = image_color.copy()\n",
        "\n",
        "# Draw SIFT keypoints\n",
        "if sift and keypoints_sift:  # Check if sift object and keypoints exist\n",
        "    cv2.drawKeypoints(\n",
        "        image_color,\n",
        "        keypoints_sift,\n",
        "        img_display_sift,\n",
        "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
        "    )\n",
        "elif not sift:  # SIFT object itself failed to create\n",
        "    cv2.putText(\n",
        "        img_display_sift,\n",
        "        \"SIFT Not Available\",\n",
        "        (50, int(img_display_sift.shape[0] / 2)),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 0, 255),\n",
        "        2,\n",
        "    )\n",
        "else:  # SIFT created but no keypoints or other error\n",
        "    cv2.putText(\n",
        "        img_display_sift,\n",
        "        \"No SIFT Keypoints\",\n",
        "        (50, int(img_display_sift.shape[0] / 2)),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 0, 255),\n",
        "        2,\n",
        "    )\n",
        "\n",
        "\n",
        "# Draw ORB keypoints\n",
        "if orb and keypoints_orb:\n",
        "    cv2.drawKeypoints(\n",
        "        image_color,\n",
        "        keypoints_orb,\n",
        "        img_display_orb,\n",
        "        color=(0, 255, 0),\n",
        "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
        "    )\n",
        "else:\n",
        "    cv2.putText(\n",
        "        img_display_orb,\n",
        "        \"No ORB Keypoints / Error\",\n",
        "        (50, int(img_display_orb.shape[0] / 2)),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),\n",
        "        2,\n",
        "    )\n",
        "\n",
        "\n",
        "# Draw AKAZE keypoints\n",
        "if akaze and keypoints_akaze:\n",
        "    cv2.drawKeypoints(\n",
        "        image_color,\n",
        "        keypoints_akaze,\n",
        "        img_display_akaze,\n",
        "        color=(255, 0, 0),\n",
        "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
        "    )\n",
        "else:\n",
        "    cv2.putText(\n",
        "        img_display_akaze,\n",
        "        \"No AKAZE Keypoints / Error\",\n",
        "        (50, int(img_display_akaze.shape[0] / 2)),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (255, 0, 0),\n",
        "        2,\n",
        "    )\n",
        "\n",
        "\n",
        "# Display using the helper function\n",
        "images_to_show = [image_color, img_display_sift, img_display_orb, img_display_akaze]\n",
        "titles_to_show = [\n",
        "    \"Original Color\",\n",
        "    f\"SIFT Keypoints ({len(keypoints_sift) if keypoints_sift else '0/Error'})\",\n",
        "    f\"ORB Keypoints ({len(keypoints_orb) if keypoints_orb else '0/Error'})\",\n",
        "    f\"AKAZE Keypoints ({len(keypoints_akaze) if keypoints_akaze else '0/Error'})\",\n",
        "]\n",
        "\n",
        "show_images(\n",
        "    images_to_show,\n",
        "    titles_to_show,\n",
        "    2,\n",
        "    2,\n",
        "    figsize=(18, 12),\n",
        "    main_title=\"Feature Detector Comparison\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da509b8e",
      "metadata": {
        "id": "da509b8e"
      },
      "source": [
        "**Discussion Questions:**\n",
        "\n",
        "1. **Quantity:** Which detector produced the most keypoints on your primary test image? Which produced the fewest? If you experimented with `nfeatures` for ORB, how did it affect the count?\n",
        "2. **Distribution:** Based on the visualization (from the provided display code):\n",
        "    * Observe the locations of the keypoints for each detector. Are they clustered in certain areas or spread out?\n",
        "    * Do they tend to be on corners, edges, or in textured regions? Describe any differences you see between SIFT, ORB, and AKAZE in terms of *where* they find features on your test image.\n",
        "3. **Keypoint Characteristics:** The visualization (if using `cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS` in the provided display code) shows the size and orientation of keypoints.\n",
        "    * Do all detectors provide scale information (indicated by circle size)?\n",
        "    * Do all detectors provide orientation information (indicated by the line radiating from the center)?\n",
        "4. **Computational Speed (Qualitative):** While not explicitly measured, did you notice any significant differences in how long each detector took to run?\n",
        "5. **Robustness (Conceptual, based on your image or recalling detector properties):**\n",
        "    * Consider an image with significant **scale changes** (e.g., an object seen from far away and then close up). Which detector (SIFT, ORB, AKAZE) would you expect to be most robust in consistently finding corresponding features? Why?\n",
        "    * Which detector is particularly known for its speed and efficiency in detecting **corners**?\n",
        "    * AKAZE uses a non-linear scale space. How might this influence the types of features it detects or its robustness compared to SIFT's Gaussian scale space?\n",
        "6. **Parameters:**\n",
        "    * Besides `nfeatures`, what other important parameters can you configure for `cv2.ORB_create()`? (Consult the OpenCV documentation).\n",
        "    * Do `cv2.SIFT_create()` or `cv2.AKAZE_create()` offer parameters to control feature detection (e.g., sensitivity, number of octaves/layers)? Briefly check their documentation.\n",
        "7. **Use Cases:** Based on your observations and understanding of their properties:\n",
        "    * Suggest a scenario or type of application where ORB might be the preferred choice.\n",
        "    * Suggest a scenario where SIFT or AKAZE might be more suitable, even if they are computationally more intensive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ec3909",
      "metadata": {
        "id": "60ec3909"
      },
      "source": [
        "# Exercise 2: Feature Matching Between Two Images\n",
        "\n",
        "**Objective:** To detect features in two images of the same scene (e.g., a building) and match them using OpenCV. This exercise will primarily use the SIFT detector, the Brute-Force (BF) matcher, apply Lowe's Ratio Test for robust matching, and visualize the results. Comparisons with AKAZE or ORB can be explored as optional extensions.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Load and Prepare Images:**\n",
        "    * Load your two images (e.g., `sticker_frontal.jpg` and `sticker_rotated.jpg`) into color variables (e.g., `image1_color`, `image2_color`).\n",
        "    * Convert both color images to grayscale (e.g., `image1_gray`, `image2_gray`). Grayscale is typically used for feature detection.\n",
        "    * Check if images loaded correctly.\n",
        "\n",
        "2.  **Initialize Detector (SIFT):**\n",
        "    * Create an SIFT detector object (e.g., using `cv2.SIFT_create()`). You can optionally specify parameters like `nfeatures` (e.g., `cv2.SIFT_create(nfeatures=1000)`).\n",
        "    * Assign it to a variable like `sift`.\n",
        "\n",
        "3.  **Detect Keypoints and Compute Descriptors for Both Images:**\n",
        "    * For the first grayscale image (`image1_gray`):\n",
        "        * Use the `sift` object's method to detect keypoints and compute their descriptors simultaneously (e.g., `sift.detectAndCompute(image1_gray, None)`).\n",
        "        * Store the results in variables like `kp1` (keypoints for image 1) and `des1` (descriptors for image 1).\n",
        "    * Repeat the process for the second grayscale image (`image2_gray`):\n",
        "        * Store its keypoints and descriptors in variables like `kp2` and `des2`.\n",
        "    * Print the number of keypoints detected in each image (e.g., using `len(kp1)`). Check if `des1` and `des2` are not `None`.\n",
        "\n",
        "4.  **Initialize Brute-Force Matcher:**\n",
        "    * Create a Brute-Force matcher object (e.g., `cv2.BFMatcher()`).\n",
        "    * Set `crossCheck=False`. We will use k-Nearest Neighbor matching and then apply Lowe's ratio test, which requires finding more than one match per keypoint. If `crossCheck=True`, the matcher only returns mutually best matches.\n",
        "\n",
        "5.  **Match Descriptors using k-NN:**\n",
        "    * Use the Brute-Force matcher's k-Nearest Neighbors matching method (e.g., `bf.knnMatch()`) to find the `k=2` best matches from `des2` for each descriptor in `des1`.\n",
        "    * Store these matches in a variable, for example, `matches_knn`. Each element in `matches_knn` will be a list containing two `DMatch` objects (the two best matches), provided at least two matches were found.\n",
        "\n",
        "6.  **Apply Lowe's Ratio Test to Filter Good Matches:**\n",
        "    * Create an empty list called `good_matches`.\n",
        "    * Iterate through each pair of matches `(m, n)` in your `matches_knn` list. `m` is the best match, `n` is the second-best match.\n",
        "    * For a match `m` to be considered \"good\", its distance should be significantly smaller than the distance of the second-best match `n`. The condition is: `m.distance < ratio * n.distance`.\n",
        "    * A common value for `ratio` is between 0.7 and 0.8. Start with `ratio = 0.75`.\n",
        "    * If a match `m` satisfies this condition, append `m` (the first, better match object) to your `good_matches` list.\n",
        "    * Print the number of raw matches found by `knnMatch` (e.g., `len(matches_knn)`) and the number of matches remaining after applying the ratio test (e.g., `len(good_matches)`).\n",
        "\n",
        "7.  **Visualize Good Matches:**\n",
        "    * Use OpenCV's function to draw the matches (e.g., `cv2.drawMatches()`).\n",
        "    * This function takes the two original color images (`image1_color`, `image2_color`), their respective keypoints (`kp1`, `kp2`), the list of good matches (`good_matches`), and an output image object (can be `None`).\n",
        "    * You can also use flags, for instance, `cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS` can make the visualization cleaner if there are many matches.\n",
        "    * Display the resulting image (which shows lines connecting matched keypoints across the two input images) using Matplotlib. Remember BGR to RGB conversion for display.\n",
        "\n",
        "**Optional Extensions:**\n",
        "* **Compare with AKAZE:**\n",
        "    1.  Replace the SIFT detector with an AKAZE detector (e.g., `akaze = cv2.AKAZE_create()`).\n",
        "    2.  Detect keypoints and compute descriptors using AKAZE for both images.\n",
        "    3.  The default AKAZE descriptors are binary (similar to ORB), so you can typically use `cv2.NORM_HAMMING` for matching. (Note: AKAZE can also produce float descriptors if configured differently).\n",
        "    4.  Apply the same `knnMatch` and Lowe's Ratio Test.\n",
        "    5.  Compare the number and visual quality of matches with those obtained using SIFT.\n",
        "\n",
        "* **Compare with SIFT (if `opencv-contrib-python` is installed):**\n",
        "    1.  Replace the SIFT detector with a ORB detector (e.g., `orb = cv2.ORB_create()`).\n",
        "    2.  Detect keypoints and compute ORB descriptors.\n",
        "    3.  The ORB descriptors are binary (similar to default in AKAZE), so you can typically use `cv2.NORM_HAMMING` for matching. (Note: AKAZE can also produce float descriptors if configured differently).\n",
        "    4.  Apply `knnMatch` and Lowe's Ratio Test as before.\n",
        "    5.  Compare with SIFT and AKAZE. Consider differences in the number of keypoints, the nature of the keypoints, computational time and match quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8e5081",
      "metadata": {
        "id": "8d8e5081"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load and Prepare Images ---\n",
        "image1_color = cv2.imread(\"images/sticker_rotated.jpg\")\n",
        "image2_color = cv2.imread(\"images/sticker_frontal.jpg\")\n",
        "\n",
        "if image1_color is None or image2_color is None:\n",
        "    print(\"Error: Could not load one or both images. Check the paths.\")\n",
        "    exit()\n",
        "\n",
        "image1_gray = cv2.cvtColor(image1_color, cv2.COLOR_BGR2GRAY)\n",
        "image2_gray = cv2.cvtColor(image2_color, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# --- 2. Initialize Detector (SIFT) ---\n",
        "# You can experiment with nfeatures; default is 500\n",
        "sift = None  # Change this\n",
        "\n",
        "# --- 3. Detect Keypoints and Compute Descriptors ---\n",
        "kp1, des1 = (None, None)  # Change this\n",
        "kp2, des2 = (None, None)  # Change this\n",
        "\n",
        "\n",
        "if des1 is None or des2 is None:\n",
        "    print(\"Error: Could not compute descriptors for one or both images.\")\n",
        "    if len(kp1) == 0:\n",
        "        print(\"No keypoints found in image 1.\")\n",
        "    if len(kp2) == 0:\n",
        "        print(\"No keypoints found in image 2.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Keypoints detected in image 1: {len(kp1)}\")\n",
        "print(f\"Keypoints detected in image 2: {len(kp2)}\")\n",
        "\n",
        "print(f\"Descriptors shape for image 1: {des1.shape}\")\n",
        "print(f\"Descriptors shape for image 2: {des2.shape}\")\n",
        "\n",
        "\n",
        "# --- 4. Initialize Brute-Force Matcher ---\n",
        "# SIFT uses real valued descriptors, so NORM_L2 is used.\n",
        "# crossCheck=False because we are using knnMatch for Lowe's ratio test.\n",
        "\n",
        "bf = None  # Change this\n",
        "\n",
        "# --- 5. Match Descriptors using k-NN ---\n",
        "# Find k=2 best matches for each descriptor in des1 from des2.\n",
        "\n",
        "matches_knn = None  # Change this\n",
        "\n",
        "print(f\"Number of raw matches (k=2) from knnMatch: {len(matches_knn)}\")\n",
        "\n",
        "# --- 6. Apply Lowe's Ratio Test ---\n",
        "good_matches = []\n",
        "ratio_thresh = 0.8  # Lowe's ratio threshold\n",
        "\n",
        "# Ensure that for each descriptor, at least two matches were found\n",
        "# (i.e., the list associated with each descriptor has length 2)\n",
        "\n",
        "good_matches = []  # Change this\n",
        "\n",
        "print(f\"Number of good matches after Lowe's ratio test: {len(good_matches)}\")\n",
        "\n",
        "# --- 7. Visualize Good Matches ---\n",
        "# cv2.drawMatches needs a list of DMatch objects.\n",
        "# The 'good_matches' list already contains these.\n",
        "\n",
        "img_matches_orb = None  # Change this\n",
        "\n",
        "# Display the matches\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.imshow(cv2.cvtColor(img_matches_orb, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Good Matches (SIFT + Lowe's Ratio Test)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280b43e5",
      "metadata": {
        "id": "280b43e5"
      },
      "source": [
        "**Discussion Questions**\n",
        "\n",
        "1.  How many keypoints were detected in `image1` and `image2` respectively?\n",
        "2.  How many raw matches were found by `knnMatch` before applying the ratio test?\n",
        "3.  How many \"good\" matches remained after applying Lowe's Ratio Test with your chosen `ratio`?\n",
        "4.  **Experiment with the `ratio` for Lowe's Test:**\n",
        "    * Change the `ratio` value (e.g., to 0.6, 0.7, 0.8, 0.9). How does this affect the number of good matches?\n",
        "    * Visually inspect the quality of matches for different ratios. What do you observe about stricter (lower ratio) vs. more lenient (higher ratio) filtering?\n",
        "5.  Why is Lowe's Ratio Test used? What problem does it help to solve in feature matching?\n",
        "6.  What does the `cv2.NORM_HAMMING` distance measure? Why is it suitable for ORB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1593de",
      "metadata": {
        "id": "0e1593de"
      },
      "source": [
        "# Exercise 3: Image Alignment with Homography\n",
        "\n",
        "**Objective:** To compute a homography matrix from matched feature points between two images and use this matrix to warp one image to align its perspective with the other. This exercise will cover the use of `cv2.findHomography` with RANSAC and `cv2.warpPerspective`.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* Python environment with OpenCV, NumPy, and Matplotlib installed.\n",
        "* **Test Images & Matches:** You will need two images of the same planar scene or an object taken with only camera rotation (e.g., `building_view1.jpg`, `building_view2.jpg`). You'll also need a set of good feature matches between them. This exercise ideally follows \"Exercise 2: Feature Matching.\" You should have:\n",
        "    * `image1_color`, `image2_color` (the original color images)\n",
        "    * `kp1`, `kp2` (keypoints for image1 and image2 respectively)\n",
        "    * `good_matches` (a list of `DMatch` objects representing good matches, e.g., after Lowe's Ratio Test)\n",
        "* Ensure you have at least 4 good matches, as this is the minimum required to compute a homography.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Prepare Matched Points (Continuing from Exercise 2):**\n",
        "    * If you haven't already, run your feature matching code (e.g., using SIFT with Lowe's Ratio Test from Exercise 2) to obtain `image1_color`, `image2_color`, `kp1`, `kp2`, and the list `good_matches`.\n",
        "    * Verify that `len(good_matches)` is at least 4. If not, you might need to adjust matching parameters (e.g., the ratio for Lowe's test, number of features) or use images with more overlap/features.\n",
        "\n",
        "2.  **Extract Coordinates of Matched Keypoints:**\n",
        "    * Create two lists (or NumPy arrays) of corresponding points:\n",
        "        * `src_pts`: Coordinates of keypoints from `good_matches` in the first image (`image1_color`). These are the points that will be transformed.\n",
        "        * `dst_pts`: Coordinates of keypoints from `good_matches` in the second image (`image2_color`). These are the target points in the reference perspective.\n",
        "    * For each `DMatch` object `m` in `good_matches`:\n",
        "        * The point in the first image is `kp1[m.queryIdx].pt`.\n",
        "        * The point in the second image is `kp2[m.trainIdx].pt`.\n",
        "    * Convert these lists of points to NumPy arrays of shape `(N, 1, 2)` and type `float32`.\n",
        "\n",
        "3.  **Compute Homography Matrix using RANSAC:**\n",
        "    * Use `cv2.findHomography()` to calculate the homography matrix `H`.\n",
        "        * `H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, reprojThresh)`\n",
        "        * `src_pts`: Source points (from `image1_color`).\n",
        "        * `dst_pts`: Destination points (from `image2_color`).\n",
        "        * `cv2.RANSAC`: Specifies the RANSAC algorithm to make the estimation robust to outliers.\n",
        "        * `reprojThresh`: Maximum allowed reprojection error for a point pair to be considered an inlier (e.g., `5.0`). Experiment with this value.\n",
        "    * The function returns the 3x3 homography matrix `H` and a `mask` array. The mask indicates which of the `good_matches` were considered inliers by RANSAC.\n",
        "    * Print the computed homography matrix `H`.\n",
        "    * Count and print the number of inliers using the `mask`.\n",
        "\n",
        "4.  **Warp the Source Image:**\n",
        "    * Use `cv2.warpPerspective()` to apply the homography `H` to `image1_color` (the source image). This will transform `image1_color` to align with the perspective of `image2_color` (the destination/reference image).\n",
        "        * `warped_image = cv2.warpPerspective(image1_color, H, (width_ref, height_ref))`\n",
        "        * `image1_color`: The image to be warped.\n",
        "        * `H`: The homography matrix.\n",
        "        * `(width_ref, height_ref)`: The dimensions (width, height) of the destination image (`image2_color`). You can get these from `image2_color.shape[1]` and `image2_color.shape[0]`.\n",
        "    * Store the result in a variable, e.g., `image1_warped_to_image2`.\n",
        "\n",
        "5.  **Visualize the Alignment:**\n",
        "    * **Option A: Side-by-Side:** Display `image2_color` (reference) and `image1_warped_to_image2` side-by-side using Matplotlib for comparison.\n",
        "    * **Option B: Overlay (Recommended for detailed check):**\n",
        "        * Create a blended image by overlaying the semi-transparent `image1_warped_to_image2` onto `image2_color`.\n",
        "        * `blended_image = cv2.addWeighted(image2_color, 0.5, image1_warped_to_image2, 0.5, 0)`\n",
        "        * Display this `blended_image` using Matplotlib."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b498f933",
      "metadata": {
        "id": "b498f933"
      },
      "source": [
        "**Discussion Questions:**\n",
        "\n",
        "1.  How many `good_matches` were initially provided to `cv2.findHomography`?\n",
        "2.  After running RANSAC, how many of these matches were considered inliers (check the `mask` or sum its values)? What does this tell you about the initial set of \"good\" matches?\n",
        "3.  Why is RANSAC crucial when computing a homography from feature matches obtained in real-world images? What kind of errors does it help mitigate?\n",
        "4.  Examine the `image1_warped_to_image2` and the `blended_image`. How accurate is the alignment?\n",
        "    * Are there any regions where the alignment is poor? What could be the reasons (e.g., non-planar parts of the scene, very few features in some areas, parallax due to camera translation and 3D structure)?\n",
        "5.  Under what two primary conditions is a homography the mathematically correct model to describe the geometric transformation between two images?\n",
        "6.  What happens if you try to compute and apply a homography between two images of a non-planar (3D) scene where the camera has undergone significant translation (not just rotation)? Would you expect a perfect global alignment? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b57545",
      "metadata": {
        "id": "c7b57545"
      },
      "outputs": [],
      "source": [
        "if image1_color is None or image2_color is None:\n",
        "    print(\"Error: Could not load one or both images. Check the paths.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Prepare Matched Points (Check minimum matches) ---\n",
        "MIN_MATCH_COUNT = 4\n",
        "if len(good_matches) >= MIN_MATCH_COUNT:\n",
        "    print(f\"Sufficient matches found: {len(good_matches)}/{MIN_MATCH_COUNT}\")\n",
        "else:\n",
        "    print(f\"Not enough matches are found - {len(good_matches)}/{MIN_MATCH_COUNT}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Extract Coordinates of Matched Keypoints ---\n",
        "# Get the coordinates of good matches in both images\n",
        "# .queryIdx is for the first image (kp1, des1)\n",
        "# .trainIdx is for the second image (kp2, des2)\n",
        "src_pts = None  # Change this\n",
        "dst_pts = None  # Change this\n",
        "\n",
        "# --- 3. Compute Homography Matrix using RANSAC ---\n",
        "# reprojThresh: Maximum reprojection error (pixels).\n",
        "# A common value is between 1.0 to 5.0.\n",
        "reproj_thresh = 5\n",
        "H, mask = (None, None)  # Change this\n",
        "\n",
        "if H is None:\n",
        "    print(\"Error: Homography could not be computed. Check your matches or images.\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nComputed Homography Matrix H:\")\n",
        "print(H)\n",
        "\n",
        "# The mask indicates inliers. Count them.\n",
        "# mask is an array of arrays, e.g. [[1], [0], [1]...]. Summing gives count of inliers.\n",
        "\n",
        "num_inliers = None  # Change this\n",
        "print(\n",
        "    f\"Number of inliers identified by RANSAC: {num_inliers} out of {len(good_matches)} original matches\"\n",
        ")\n",
        "\n",
        "# --- 4. Warp the Source Image ---\n",
        "# We want to warp image1 to align with image2's perspective.\n",
        "# The size of the output image should be the size of image2.\n",
        "height_ref, width_ref = image2_color.shape[:2]\n",
        "image1_warped_to_image2 = None  # Change this\n",
        "\n",
        "# --- 5. Visualize the Alignment ---\n",
        "\n",
        "# Option A: Side-by-Side\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
        "axs[0].imshow(cv2.cvtColor(image2_color, cv2.COLOR_BGR2RGB))\n",
        "axs[0].set_title(\"Reference Image (Image 2)\")\n",
        "axs[0].axis(\"off\")\n",
        "\n",
        "axs[1].imshow(cv2.cvtColor(image1_warped_to_image2, cv2.COLOR_BGR2RGB))\n",
        "axs[1].set_title(\"Image 1 Warped to Image 2 Perspective\")\n",
        "axs[1].axis(\"off\")\n",
        "plt.suptitle(\"Homography Alignment: Side-by-Side\")\n",
        "plt.show()\n",
        "\n",
        "# Option B: Overlay\n",
        "alpha = 0.6  # Transparency of the warped image\n",
        "beta = 1.0 - alpha  # Transparency of the reference image\n",
        "blended_image = cv2.addWeighted(image2_color, alpha, image1_warped_to_image2, beta, 0.0)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(cv2.cvtColor(blended_image, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Blended Overlay: Reference Image and Warped Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# (Optional) Draw inlier matches\n",
        "# Create a new image drawing only the inlier matches\n",
        "# The mask is used by drawMatches to draw only inlier points.\n",
        "# Note: mask elements are 0 or 1. Need to convert to list of lists for drawMatches.\n",
        "matches_mask_for_drawing = mask.ravel().tolist()\n",
        "\n",
        "img_inlier_matches = cv2.drawMatches(\n",
        "    image1_color,\n",
        "    kp1,\n",
        "    image2_color,\n",
        "    kp2,\n",
        "    [m for i, m in enumerate(good_matches) if mask[i][0] == 1],\n",
        "    None,\n",
        "    # matchColor=(0, 255, 0), # Green for inliers\n",
        "    # singlePointColor=None,\n",
        "    # matchesMask=matches_mask_for_drawing, # This would require good_matches itself\n",
        "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.imshow(cv2.cvtColor(img_inlier_matches, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Inlier Matches (RANSAC)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e7fdee",
      "metadata": {
        "id": "91e7fdee"
      },
      "source": [
        "**Discussion Questions:**\n",
        "\n",
        "1.  How many `good_matches` were initially provided to `cv2.findHomography`?\n",
        "2.  After running RANSAC, how many of these matches were considered inliers (check the `mask` or sum its values)? What does this tell you about the initial set of \"good\" matches?\n",
        "3.  Why is RANSAC crucial when computing a homography from feature matches obtained in real-world images? What kind of errors does it help mitigate?\n",
        "4.  Examine the `image1_warped_to_image2` and the `blended_image`. How accurate is the alignment?\n",
        "    * Are there any regions where the alignment is poor? What could be the reasons (e.g., non-planar parts of the scene, very few features in some areas, parallax due to camera translation and 3D structure)?\n",
        "5.  Under what two primary conditions is a homography the mathematically correct model to describe the geometric transformation between two images?\n",
        "6.  What happens if you try to compute and apply a homography between two images of a non-planar (3D) scene where the camera has undergone significant translation (not just rotation)? Would you expect a perfect global alignment? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c04608",
      "metadata": {
        "id": "e2c04608"
      },
      "source": [
        "# Exercise 4: Measuring Lengths on a Plane using Homography\n",
        "\n",
        "**Objective:** To use a homography transformation to undo perspective distortion in an image and perform metric measurements of an object (a line) on a known plane (a piece of paper).\n",
        "\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Load the Image:**\n",
        "    * Load the image (e.g., `line.jpg`) into your Python script using `cv2.imread()`.\n",
        "\n",
        "2.  **Define Image Coordinates (Pixel Coordinates):**\n",
        "    * You need to identify the pixel coordinates of specific points in your *distorted* image:\n",
        "        * The **four corners of the sheet of paper**. List them in a consistent order (e.g., top-left, top-right, bottom-right, bottom-left).\n",
        "        * The **two endpoints of the line** you want to measure.\n",
        "    * **How to get these coordinates:**\n",
        "        * **Option A (Manual Estimation):** Open the image in an image viewer that shows pixel coordinates as you hover your mouse (e.g., `plotly.express.imshow()` and then observe coordinates). Carefully note these down.\n",
        "        * **Option B (Interactive Clicking - Advanced):** Write a small OpenCV script with a mouse callback function (`cv2.setMouseCallback`) to click on the points and record their coordinates.\n",
        "    * Store these image coordinates as NumPy arrays. For the square, it will be a `4x2` array. For the line, a `2x2` array.\n",
        "\n",
        "3.  **Define Real-World Coordinates of the Reference Square:**\n",
        "    * Define the known real-world coordinates for the four corners of the A4 sheet of paper. These coordinates should correspond to the order you used for the image points.\n",
        "    * For a 5x5cm square, if you want to measure in millimeters (mm), you could define the corners as:\n",
        "        * Top-left: `(0, 0)`\n",
        "        * Top-right: `(50, 0)` (assuming width is 50mm)\n",
        "        * Bottom-right: `(50, 50)`\n",
        "        * Bottom-left: `(0, 50)` (assuming height is 50mm)\n",
        "    * Store these as a NumPy array, e.g., `world_pts_sheet`.\n",
        "\n",
        "4.  **Compute the Homography Matrix (Image to World):**\n",
        "    * Use `cv2.findHomography()` to find the perspective transformation matrix `H` that maps the *image coordinates* of the square's corners (`image_pts_sheet`) to their *real-world coordinates* (`world_pts_sheet`).\n",
        "        * `H, status = cv2.findHomography(image_pts_sheet, world_pts_sheet)`\n",
        "        * Note: We are mapping *from* the distorted image *to* the ideal, flat-plane world representation.\n",
        "\n",
        "5.  **Transform Line Endpoints to Real-World Coordinates:**\n",
        "    * You have the image coordinates of the line's endpoints (`image_pts_line`).\n",
        "    * Use `cv2.perspectiveTransform(src, H)` to apply the computed homography `H` to these image points.\n",
        "        * `src` must be a 3-channel array (or an array of 2-channel arrays). So, reshape your `image_pts_line` (e.g., `(N, 1, 2)` where N is number of points).\n",
        "        * The result will be the coordinates of the line's endpoints in your defined real-world coordinate system (e.g., in millimeters on the plane of the paper).\n",
        "    * Store these transformed points, e.g., `world_pts_line_transformed`.\n",
        "\n",
        "6.  **Calculate the Length of the Line in Real-World Units:**\n",
        "    * You now have the two endpoints of the line in real-world coordinates (e.g., `(x1_w, y1_w)` and `(x2_w, y2_w)`).\n",
        "    * Calculate the Euclidean distance between these two points:\n",
        "        * `length = sqrt((x2_w - x1_w)^2 + (y2_w - y1_w)^2)`\n",
        "    * This `length` is your measurement of the line in the units you defined for your real-world square (e.g., millimeters).\n",
        "\n",
        "7.  **Verification:**\n",
        "    * Compare the actual length (150 mm) to the length calculated using the homography.\n",
        "\n",
        "**Optional Visualization:**\n",
        "\n",
        "* You can use the inverse of the homography `H_inv = np.linalg.inv(H)` (or compute a new homography from world to image) to warp the entire input image so that the sheet of paper appears perfectly rectangular (\"bird's-eye view\"). Then, you can draw your measured line on this rectified image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3170a2",
      "metadata": {
        "id": "0b3170a2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "image_path = \"images/line.jpg\"  # Replace with your image path\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "if img is None:\n",
        "    print(f\"Error: Could not load image at {image_path}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Image loaded successfully. Shape: {img.shape}\")\n",
        "\n",
        "# Example coordinates (these are placeholders - YOU MUST CHANGE THEM)\n",
        "image_pts_sheet = np.array(\n",
        "    [\n",
        "        [150, 180],  # Top-Left corner of the square in the image\n",
        "        [450, 150],  # Top-Right corner\n",
        "        [480, 400],  # Bottom-Right corner\n",
        "        [120, 430],  # Bottom-Left corner\n",
        "    ],\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "# Example coordinates for the line to be measured (YOU MUST CHANGE THEM)\n",
        "image_pts_line = np.array(\n",
        "    [\n",
        "        [200, 250],  # Start point of the line in the image\n",
        "        [400, 350],  # End point of the line in the image\n",
        "    ],\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "print(\"\\nImage coordinates for reference square (pixels):\")\n",
        "print(image_pts_sheet)\n",
        "print(\"\\nImage coordinates for line endpoints (pixels):\")\n",
        "print(image_pts_line)\n",
        "\n",
        "# --- 3. Define Real-World Coordinates of the Reference Square ---\n",
        "# We want measurements in millimeters.\n",
        "# Order must match the image_pts_sheet order.\n",
        "world_pts_sheet = None  # Change this\n",
        "\n",
        "print(\"\\nReal-world coordinates for reference square (mm):\")\n",
        "print(world_pts_sheet)\n",
        "\n",
        "# --- 4. Compute the Homography Matrix (Image to World) ---\n",
        "# This matrix will transform points from the image plane to the real-world plane.\n",
        "H, status = (None, None)  # Change this\n",
        "\n",
        "if H is None:\n",
        "    print(\"Error: Homography computation failed. Check your points.\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nComputed Homography Matrix (Image to World):\")\n",
        "print(H)\n",
        "\n",
        "# --- 5. Transform Line Endpoints to Real-World Coordinates ---\n",
        "# cv2.perspectiveTransform expects points in a specific shape: (N, 1, 2)\n",
        "# where N is the number of points.\n",
        "image_pts_line_reshaped = image_pts_line.reshape(-1, 1, 2)\n",
        "\n",
        "world_pts_line_transformed = None  # Change this\n",
        "\n",
        "if world_pts_line_transformed is None:\n",
        "    print(\"Error: Perspective transform failed for line points.\")\n",
        "    exit()\n",
        "\n",
        "# Reshape back for easier access if needed, e.g., N x 2\n",
        "world_pts_line_transformed = world_pts_line_transformed.reshape(-1, 2)\n",
        "print(\"\\nTransformed real-world coordinates for line endpoints (mm):\")\n",
        "print(world_pts_line_transformed)\n",
        "\n",
        "# --- 6. Calculate the Length of the Line in Real-World Units ---\n",
        "length_in_mm = None  # Change this\n",
        "\n",
        "print(f\"\\nCalculated length of the line: {length_in_mm:.2f} mm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d699934d",
      "metadata": {
        "id": "d699934d"
      },
      "source": [
        "# Exercise 5: Robustness to Outliers - Homography Before & After RANSAC\n",
        "\n",
        "**Objective:** To observe and understand the impact of outlier matches on homography computation and to demonstrate the effectiveness of RANSAC in mitigating these effects.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Load Images and Good Matches:**\n",
        "    * If you haven't already, run your feature matching code (e.g., using ORB with Lowe's Ratio Test from Exercise 2) to obtain `image1_color`, `image2_color`, `kp1`, `kp2`, and the list `good_matches`.\n",
        "\n",
        "2.  **Introduce Outlier (Bad) Matches:**\n",
        "    * Create a new list of matches, say `matches_with_outliers`, by copying your `good_matches`.\n",
        "    * **Deliberately add a few incorrect matches** to this new list. For example, take 3-5 keypoints from `image1_color` (using their indices from `kp1`) and pair them with completely unrelated keypoints from `image2_color` (using random or deliberately wrong indices from `kp2`).\n",
        "    * To do this, you'll create new `cv2.DMatch` objects for these bad pairings and append them to `matches_with_outliers`.\n",
        "        * A `cv2.DMatch` object typically takes `_queryIdx`, `_trainIdx`, and `_distance`. For these fake outliers, the distance can be set to a small arbitrary value if needed by some functions, though it won't be used by `findHomography`'s core logic when method 0 is used.\n",
        "    * Print the total number of matches in `matches_with_outliers`.\n",
        "\n",
        "3.  **Extract Coordinates for All Matches (including Outliers):**\n",
        "    * From `matches_with_outliers`, extract the source points (`src_pts_all`) from `kp1` and destination points (`dst_pts_all`) from `kp2`.\n",
        "    * Ensure these are NumPy arrays of shape `(N, 1, 2)` and type `float32`.\n",
        "\n",
        "4.  **Compute Homography WITHOUT RANSAC:**\n",
        "    * Use `cv2.findHomography()` with `method=0`. This method uses a simple least-squares approach on *all* provided points.\n",
        "        * `H_no_ransac, _ = cv2.findHomography(src_pts_all, dst_pts_all, 0)`\n",
        "    * Print the computed `H_no_ransac`.\n",
        "\n",
        "5.  **Warp Image using Homography WITHOUT RANSAC:**\n",
        "    * Use `cv2.warpPerspective()` to apply `H_no_ransac` to `image1_color`.\n",
        "        * `image1_warped_no_ransac = cv2.warpPerspective(image1_color, H_no_ransac, (image2_color.shape[1], image2_color.shape[0]))`\n",
        "    * Display `image2_color` (reference) and `image1_warped_no_ransac` side-by-side or blended. Observe the (likely poor) alignment.\n",
        "\n",
        "6.  **Compute Homography WITH RANSAC:**\n",
        "    * Now, use `cv2.findHomography()` with the `cv2.RANSAC` method and a `reprojThresh` (e.g., 5.0).\n",
        "        * `H_ransac, mask_ransac = cv2.findHomography(src_pts_all, dst_pts_all, cv2.RANSAC, 5.0)`\n",
        "    * Print the computed `H_ransac` and the number of inliers found (by summing `mask_ransac`).\n",
        "\n",
        "7.  **Warp Image using Homography WITH RANSAC:**\n",
        "    * Use `cv2.warpPerspective()` to apply `H_ransac` to `image1_color`.\n",
        "        * `image1_warped_ransac = cv2.warpPerspective(image1_color, H_ransac, (image2_color.shape[1], image2_color.shape[0]))`\n",
        "    * Display `image2_color` (reference) and `image1_warped_ransac` side-by-side or blended. Observe the (hopefully good) alignment.\n",
        "\n",
        "8.  **Visualize Inlier/Outlier Mask:**\n",
        "    * Use `cv2.drawMatches()` to visualize the matches from `matches_with_outliers`.\n",
        "    * Use the `mask_ransac` (obtained from `findHomography` with RANSAC) as the `matchesMask` parameter in `cv2.drawMatches()`. This will draw lines only for inlier matches, or you can customize it to draw inliers and outliers in different colors.\n",
        "    * `img_matches_with_mask = cv2.drawMatches(image1_color, kp1, image2_color, kp2, matches_with_outliers, None, matchesMask=mask_ransac.ravel().tolist(), flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)`\n",
        "    * Display this image.\n",
        "\n",
        "**Discussion Points:**\n",
        "\n",
        "1.  Describe the visual result of `image1_warped_no_ransac`. How did the outliers affect the homography computation when RANSAC was not used?\n",
        "2.  Describe the visual result of `image1_warped_ransac`. How did RANSAC improve the result?\n",
        "3.  Examine the `mask_ransac`. How many of your deliberately added bad matches were correctly identified as outliers by RANSAC? Were any of the original \"good\" matches (if you can distinguish them) classified as outliers? Why might that happen?\n",
        "4.  What is the fundamental principle behind RANSAC that allows it to be robust to outliers?\n",
        "5.  If you increased the number of outliers significantly (e.g., 50% of matches are bad), how might that affect RANSAC's ability to find the correct homography? What parameter in `cv2.findHomography` relates to the number of iterations RANSAC performs, and why is it important?\n",
        "6.  Besides `method=0` and `cv2.RANSAC`, what other methods are available for `cv2.findHomography` (e.g., `cv2.LMEDS`)? When might they be useful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80cd2963",
      "metadata": {
        "id": "80cd2963"
      },
      "outputs": [],
      "source": [
        "def add_outliers_to_keypoints(matches):\n",
        "    import random\n",
        "\n",
        "    matches_with_outliers = list(good_matches)  # Start with a copy of good matches\n",
        "    num_outliers_to_add = 5  # Let's add 5 bad matches\n",
        "\n",
        "    if len(kp1) < num_outliers_to_add or len(kp2) < num_outliers_to_add:\n",
        "        print(\n",
        "            \"Warning: Not enough keypoints to select distinct outliers. Reducing number of outliers.\"\n",
        "        )\n",
        "        num_outliers_to_add = min(len(kp1), len(kp2), num_outliers_to_add)\n",
        "\n",
        "    # Ensure we have enough keypoints in kp1 and kp2 to pick from\n",
        "    # And that we don't pick existing trainIdx for a queryIdx if possible (though not strictly enforced here for simplicity)\n",
        "    for i in range(num_outliers_to_add):\n",
        "        if not kp1 or not kp2:\n",
        "            break  # Should not happen if previous checks passed\n",
        "\n",
        "        # Pick a random keypoint from image 1 (query)\n",
        "        idx_kp1 = random.randint(0, len(kp1) - 1)\n",
        "\n",
        "        # Pick a completely random keypoint from image 2 (train) for the bad match\n",
        "        # This chosen kp2 index should ideally not be the \"correct\" match for idx_kp1\n",
        "        idx_kp2_bad = random.randint(0, len(kp2) - 1)\n",
        "\n",
        "        # Create a new DMatch object for the bad match\n",
        "        # Distance can be arbitrary for this demonstration, let's make it look \"good\" to fool least-squares\n",
        "        bad_match = cv2.DMatch(\n",
        "            _queryIdx=idx_kp1, _trainIdx=idx_kp2_bad, _distance=random.uniform(10, 30)\n",
        "        )\n",
        "        matches_with_outliers.append(bad_match)\n",
        "        print(f\"Added outlier: kp1[{idx_kp1}] matched with kp2[{idx_kp2_bad}]\")\n",
        "    return matches_with_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa63befc",
      "metadata": {
        "id": "fa63befc"
      },
      "outputs": [],
      "source": [
        "# --- (Assuming SIFT feature matching from Exercise 2 is done here) ---\n",
        "\n",
        "# Use keypoints from above\n",
        "\n",
        "# --- 2. Introduce Outlier (Bad) Matches ---\n",
        "matches_with_outliers = add_outliers_to_keypoints(good_matches)\n",
        "\n",
        "print(f\"Total matches (good + outliers): {len(matches_with_outliers)}\")\n",
        "\n",
        "# --- 3. Extract Coordinates for All Matches (including Outliers) ---\n",
        "\n",
        "src_pts_all = None  # Change this\n",
        "dst_pts_all = None  # Change this\n",
        "\n",
        "# --- 4. Compute Homography WITHOUT RANSAC ---\n",
        "# Method 0 is the default least-squares method using all points\n",
        "H_no_ransac, status_no_ransac = None\n",
        "\n",
        "print(\"\\nHomography Matrix WITHOUT RANSAC (method=0):\")\n",
        "if H_no_ransac is not None:\n",
        "    print(H_no_ransac)\n",
        "else:\n",
        "    print(\"Failed to compute homography without RANSAC.\")\n",
        "    # We can still try RANSAC if this fails\n",
        "\n",
        "# --- 5. Warp Image using Homography WITHOUT RANSAC ---\n",
        "if H_no_ransac is not None:\n",
        "    image1_warped_no_ransac = cv2.warpPerspective(\n",
        "        image1_color, H_no_ransac, (image2_color.shape[1], image2_color.shape[0])\n",
        "    )\n",
        "\n",
        "    fig_no_ransac, axs_no_ransac = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axs_no_ransac[0].imshow(cv2.cvtColor(image2_color, cv2.COLOR_BGR2RGB))\n",
        "    axs_no_ransac[0].set_title(\"Reference Image 2\")\n",
        "    axs_no_ransac[0].axis(\"off\")\n",
        "    axs_no_ransac[1].imshow(cv2.cvtColor(image1_warped_no_ransac, cv2.COLOR_BGR2RGB))\n",
        "    axs_no_ransac[1].set_title(\"Warped Image 1 (NO RANSAC)\")\n",
        "    axs_no_ransac[1].axis(\"off\")\n",
        "    plt.suptitle(\"Alignment WITHOUT RANSAC\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping warp without RANSAC as H_no_ransac is None.\")\n",
        "\n",
        "\n",
        "# --- 6. Compute Homography WITH RANSAC ---\n",
        "reproj_thresh = 5.0  # Maximum reprojection error (pixels) for RANSAC\n",
        "H_ransac, mask_ransac = None  # Change this\n",
        "\n",
        "print(\"\\nHomography Matrix WITH RANSAC:\")\n",
        "if H_ransac is not None:\n",
        "    print(H_ransac)\n",
        "    num_inliers = np.sum(mask_ransac)\n",
        "    print(\n",
        "        f\"Number of inliers found by RANSAC: {num_inliers} out of {len(matches_with_outliers)}\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Failed to compute homography WITH RANSAC. Check matches or reproj_thresh.\")\n",
        "    exit()  # Critical if this fails\n",
        "\n",
        "# --- 7. Warp Image using Homography WITH RANSAC ---\n",
        "image1_warped_ransac = cv2.warpPerspective(\n",
        "    image1_color, H_ransac, (image2_color.shape[1], image2_color.shape[0])\n",
        ")\n",
        "\n",
        "fig_ransac, axs_ransac = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axs_ransac[0].imshow(cv2.cvtColor(image2_color, cv2.COLOR_BGR2RGB))\n",
        "axs_ransac[0].set_title(\"Reference Image 2\")\n",
        "axs_ransac[0].axis(\"off\")\n",
        "axs_ransac[1].imshow(cv2.cvtColor(image1_warped_ransac, cv2.COLOR_BGR2RGB))\n",
        "axs_ransac[1].set_title(\"Warped Image 1 (WITH RANSAC)\")\n",
        "axs_ransac[1].axis(\"off\")\n",
        "plt.suptitle(\"Alignment WITH RANSAC\")\n",
        "plt.show()\n",
        "\n",
        "# --- 8. Visualize Inlier/Outlier Mask ---\n",
        "# drawMatches will use the mask to draw only inliers if matchesMask is provided.\n",
        "# To draw all matches but color them differently would require more custom drawing.\n",
        "# Here, we'll just draw the inliers identified by RANSAC.\n",
        "if mask_ransac is not None:\n",
        "    # Create a list of DMatch objects that are inliers\n",
        "    inlier_matches = [\n",
        "        m for i, m in enumerate(matches_with_outliers) if mask_ransac[i][0] == 1\n",
        "    ]\n",
        "\n",
        "    img_inlier_matches = cv2.drawMatches(\n",
        "        image1_color,\n",
        "        kp1,\n",
        "        image2_color,\n",
        "        kp2,\n",
        "        inlier_matches,\n",
        "        None,  # Pass only inlier DMatch objects\n",
        "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
        "    )\n",
        "\n",
        "    # Alternative: draw all matches and use matchesMask parameter\n",
        "    # img_matches_with_mask = cv2.drawMatches(image1_color, kp1, image2_color, kp2,\n",
        "    #                                         matches_with_outliers, None,\n",
        "    #                                         matchesMask=mask_ransac.ravel().tolist(), # Mask for all matches\n",
        "    #                                         flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.imshow(cv2.cvtColor(img_inlier_matches, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Inlier Matches Identified by RANSAC ({len(inlier_matches)} inliers)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"mask_ransac is None, skipping inlier visualization.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3dad",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}