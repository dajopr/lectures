{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajopr/lectures/blob/main/image_processing/lecture_08_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80850219",
      "metadata": {
        "id": "80850219"
      },
      "source": [
        "\n",
        "# Exercise Sheet: Image Classification - From Feature Engineering to Neural Networks\n",
        "\n",
        "**Prerequisites**: Python, NumPy, OpenCV, Matplotlib. Familiarity with image processing fundamentals and basic machine learning concepts.\n",
        "\n",
        "**Goal**: To build a practical understanding of image classification by progressing from classical machine learning to neural networks. This journey involves implementing feature engineering (LBP, SIFT), applying various classifiers (Naive Bayes, SVM, k-Means), and ultimately building, training, and tuning neural networks in PyTorch to solve diverse classification problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4266efca",
      "metadata": {
        "id": "4266efca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "SHAPES_DIR = \"images/shape_dataset\"\n",
        "VEHICLES_DIR = \"images/vehicles\"\n",
        "\n",
        "\n",
        "def load_dataset(dataset=\"shapes\", split=\"train\"):\n",
        "    \"\"\"\n",
        "    Loads images from the dataset directory, flattens them, and creates labels.\n",
        "\n",
        "    Args:\n",
        "        dataset_dir (str): The path to the dataset directory.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the data (flattened images) and labels.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    if dataset == \"shapes\":\n",
        "        # Define class names and their corresponding integer labels\n",
        "        class_map = {\"circles\": 0, \"squares\": 1}\n",
        "\n",
        "        for class_name, label in class_map.items():\n",
        "            class_dir = os.path.join(SHAPES_DIR, class_name)\n",
        "            if not os.path.isdir(class_dir):\n",
        "                print(f\"Warning: Directory not found at {class_dir}\")\n",
        "                continue\n",
        "\n",
        "            for filename in os.listdir(class_dir):\n",
        "                if filename.endswith(\".png\"):\n",
        "                    img_path = os.path.join(class_dir, filename)\n",
        "                    # Read the image in grayscale mode\n",
        "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                    if img is not None:\n",
        "                        # Flatten the 28x28 image to a 784-element vector\n",
        "                        flattened_img = img.flatten()\n",
        "                        data.append(flattened_img)\n",
        "                        labels.append(label)\n",
        "\n",
        "        # Convert lists to NumPy arrays for efficient computation\n",
        "        return np.array(data), np.array(labels)\n",
        "    elif dataset == \"vehicles\":\n",
        "        dir = os.path.join(VEHICLES_DIR, split)\n",
        "        for label, category in enumerate(os.listdir(dir)):\n",
        "            category_path = os.path.join(dir, category)\n",
        "            if not os.path.isdir(category_path):\n",
        "                continue\n",
        "            for filename in os.listdir(category_path):\n",
        "                img_path = os.path.join(category_path, filename)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                img = cv2.resize(img, (224, 224))\n",
        "                if img is not None:\n",
        "                    data.append(img)\n",
        "                    labels.append(category)  # Store category name as label\n",
        "        return data, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b338de34",
      "metadata": {
        "id": "b338de34"
      },
      "source": [
        "## Exercise 1: k-Nearest Neighbor Classification\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Load the dataset using load_dataset\n",
        "2. Split the dataset into train and test subsets using train_test_split\n",
        "3. Use KNeighborsClassifier to fit a kNN Classifier\n",
        "4. Test the classifier using the test set\n",
        "5. Repeat steps 3 and 4 for different values of k\n",
        "\n",
        "**Discussion questions**\n",
        "1. What is the specific role of the k in k-NN? What happens when k=1?\n",
        "2. Impact of 'k': Based on the experiment, which value of k gave you the highest accuracy? Why do you think that specific value performed best for this dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "251dbe07",
      "metadata": {
        "id": "251dbe07"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1adc1bbb",
      "metadata": {
        "id": "1adc1bbb"
      },
      "source": [
        "## Exercise 2: Scene Classification with Local Binary Patterns (LBP)\n",
        "\n",
        "In our previous exercise, we used global geometric features to classify shapes. Now, we'll shift our focus to another fundamental aspect of an image: **texture**. Different scenes, like a sandy beach or a dense forest, have distinct textural properties. We can exploit these differences for classification.\n",
        "\n",
        "Our goal is to **classify images into \"beach\" or \"forest\" categories based solely on their texture**.\n",
        "\n",
        "To do this, we will use a powerful texture descriptor called **Local Binary Patterns (LBP)**.\n",
        "\n",
        "**Tasks**:\n",
        "\n",
        "1.  **Feature Extraction Pipeline**: For each image in the dataset, you need to perform the following steps:\n",
        "    * Load the image and convert it to **grayscale**. Texture is a property of luminance, not color.\n",
        "    * Compute the **LBP representation** of the grayscale image. The `skimage.feature.local_binary_pattern` function is perfect for this. This creates a new 2D array where pixel values represent local texture patterns.\n",
        "    * Calculate a **histogram** of the LBP image's pixel values. This histogram serves as our feature vector. A histogram with 256 bins is standard, as an 8-point LBP creates 2^8 = 256 possible patterns.\n",
        "    * Store these histograms (features) and their corresponding labels (\"beach\" or \"forest\").\n",
        "\n",
        "2.  **Train and Evaluate a Classifier**:\n",
        "    * Split your extracted features and labels into a training set and a testing set.\n",
        "    * Train a **Naive Bayes classifier** (`sklearn.naive_bayes.GaussianNB`) on the training data.\n",
        "    * Evaluate the classifier's performance on the test data. Calculate and print metrics like accuracy to see how well it performs.\n",
        "\n",
        "**Discussion Questions**\n",
        "\n",
        "1.  **Interpreting the Results**: Look at the classification report. How well did the model perform in terms of accuracy, precision, and recall for each class? Is one class easier to identify than the other? Why might that be?\n",
        "\n",
        "2.  **The Power of LBP**: Why is a histogram of Local Binary Patterns a good feature for describing textures like sand, water, leaves, and tree bark? What kind of image information does LBP capture?\n",
        "\n",
        "3.  **LBP Parameters**: In the code, `radius` and `n_points` are key parameters for LBP. What do these parameters control? How do you think changing them (e.g., a smaller radius or fewer points) would affect the feature vector and the final classification accuracy?\n",
        "\n",
        "4.  **Choice of Classifier**: We used a Naive Bayes classifier. What is the core assumption that a \"naive\" Bayes classifier makes? Why might this be a reasonable (or perhaps flawed) assumption for LBP histogram features?\n",
        "\n",
        "5.  **From Texture to Scene**: Our model only \"sees\" texture, not objects or shapes. How does this compare to the shape classification exercise? What are the limitations of a purely texture-based approach for general scene understanding?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4368a79",
      "metadata": {
        "id": "a4368a79"
      },
      "outputs": [],
      "source": [
        "# --- 1. Feature Extraction ---\n",
        "\n",
        "\n",
        "def extract_lbp_features(data_dir):\n",
        "    \"\"\"\n",
        "    Iterates through subdirectories of data_dir, extracts LBP features\n",
        "    for each image, and returns features and labels.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # LBP parameters\n",
        "    radius = 3\n",
        "    n_points = 8 * radius\n",
        "\n",
        "    # Loop through each category (beach, forest)\n",
        "    for category in os.listdir(data_dir):\n",
        "        category_dir = os.path.join(data_dir, category)\n",
        "        if not os.path.isdir(category_dir):\n",
        "            continue\n",
        "\n",
        "        label = 1 if category == \"forest\" else 0\n",
        "\n",
        "        # Loop through images in the category\n",
        "        for filename in os.listdir(category_dir):\n",
        "            img_path = os.path.join(category_dir, filename)\n",
        "\n",
        "            # Read and convert to grayscale\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                continue\n",
        "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Compute LBP\n",
        "            lbp = None  # Use local_binary_pattern\n",
        "\n",
        "            # Compute histogram of LBP\n",
        "            # n_bins should be n_points + 2 for the 'uniform' method\n",
        "            n_bins = int(lbp.max() + 1)\n",
        "            (hist, _) = None  # use np.histogram()\n",
        "\n",
        "            # Normalize the histogram to make it comparable across images\n",
        "            hist = hist.astype(\"float\")\n",
        "            hist /= hist.sum() + 1e-6  # Add a small epsilon to avoid division by zero\n",
        "\n",
        "            # Append features and label\n",
        "            features.append(hist)\n",
        "            labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "\n",
        "# Path to your dataset\n",
        "DATA_DIR = \"dataset\"\n",
        "\n",
        "print(\"Starting feature extraction...\")\n",
        "features, labels = extract_lbp_features(DATA_DIR)\n",
        "print(f\"Feature extraction complete. Extracted {len(features)} feature vectors.\")\n",
        "print(f\"Feature vector shape: {features.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "\n",
        "# --- 2. Train and Evaluate the Classifier ---\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.20, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Initialize and train the Gaussian Naive Bayes model\n",
        "model = None  # Implement this\n",
        "\n",
        "\n",
        "# --- 3. Evaluation ---\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Print a detailed classification report\n",
        "# target_names=['beach', 'forest'] maps the labels (0, 1) to their names\n",
        "\n",
        "report = None  # Create classification_report\n",
        "\n",
        "print(report)\n",
        "\n",
        "\n",
        "# Visualize a sample LBP image and its histogram\n",
        "def visualize_sample_lbp():\n",
        "    sample_image_path = os.path.join(\n",
        "        DATA_DIR, \"beach\", os.listdir(os.path.join(DATA_DIR, \"beach\"))[0]\n",
        "    )\n",
        "    image = cv2.imread(sample_image_path)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    radius = 3\n",
        "    n_points = 8 * radius\n",
        "    lbp = local_binary_pattern(gray, n_points, radius, method=\"uniform\")\n",
        "    n_bins = int(lbp.max() + 1)\n",
        "    hist, _ = np.histogram(\n",
        "        lbp.ravel(), bins=np.arange(0, n_bins + 1), range=(0, n_bins)\n",
        "    )\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Original Grayscale\")\n",
        "    plt.imshow(gray, cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"LBP Representation\")\n",
        "    plt.imshow(lbp, cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"LBP Histogram (Feature Vector)\")\n",
        "    plt.bar(range(len(hist)), hist, width=1.0)\n",
        "    plt.xlabel(\"LBP Bins\")\n",
        "    plt.ylabel(\"Normalized Frequency\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"\\nVisualizing a sample LBP transformation...\")\n",
        "visualize_sample_lbp()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf85640e",
      "metadata": {
        "id": "cf85640e"
      },
      "source": [
        "## Exercise 3: Object Classification with SIFT and SVM\n",
        "\n",
        "Welcome to the third exercise! We are moving from classifying general scenes by texture to identifying specific objects within images. Our goal is to build a robust classifier that can distinguish between **\"buses\"** and **\"motorbikes\"**.\n",
        "\n",
        "To achieve this, we will use the **Scale-Invariant Feature Transform (SIFT)** to find unique keypoints on the objects. Since each image will have a variable number of these keypoints, we can't directly feed them into a classifier. We will solve this using the **Bag of Visual Words (BoVW)** model, a powerful technique for converting local features into a fixed-size vector representation for any image.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "You will implement the full Bag of Visual Words pipeline.\n",
        "\n",
        "**Step 1: SIFT Feature Extraction**\n",
        "- For every image in your **training set**, detect SIFT keypoints and compute their 128-dimensional descriptor vectors.\n",
        "- Since each image will have a different number of keypoints, you will end up with a large collection of descriptors from all training images.\n",
        "\n",
        "**Step 2: Create a \"Visual Vocabulary\"**\n",
        "- Pool all the SIFT descriptors from all training images into one large list.\n",
        "- Use **k-Means clustering** on this list of descriptors to group them into `k` clusters. A good starting point is `k=50`.\n",
        "- The `k` cluster centers are your **\"visual words.\"** Together, they form the vocabulary. Each visual word represents a common type of local feature (e.g., a wheel edge, a wingtip, a handlebar) found in your training images.\n",
        "\n",
        "**Step 3: Represent Images as Feature Histograms**\n",
        "- Now, you need a way to represent each image using a single, fixed-length vector. For every image (both **training and testing**):\n",
        "    - Extract its SIFT descriptors.\n",
        "    - For each descriptor, find the closest \"visual word\" (cluster center) from your vocabulary.\n",
        "    - Create a **histogram of size k**. Count how many times each of the `k` visual words appears in the image.\n",
        "- This histogram is your new feature vector for the image. Every image, regardless of its size or number of keypoints, is now represented by a single vector of length `k`.\n",
        "\n",
        "**Step 4: Train and Evaluate the SVM Classifier**\n",
        "- Train a **Support Vector Machine (SVM)** classifier using the feature histograms of your training images and their corresponding labels (\"bus\" or \"motorbike\").\n",
        "- Use the trained model to predict the labels for the feature histograms of your **test images**.\n",
        "- Evaluate the classifier's performance and analyze the results.\n",
        "\n",
        "**Discussion Questions**\n",
        "\n",
        "1.  **The Visual Vocabulary (`k`)**: We chose `k=50` for our vocabulary size. What does this `k` represent in the context of our images? What do you think would happen if you chose a very small `k` (e.g., 5) or a very large `k` (e.g., 500)? How might it affect performance and computation time?\n",
        "\n",
        "2.  **SIFT vs. LBP**: Compare the SIFT features used in this exercise with the LBP features from the previous one. Why is SIFT better suited for *object recognition* (airplanes vs. motorbikes), while LBP was effective for *scene classification* (beach vs. forest)?\n",
        "\n",
        "3.  **The Role of SVM**: Why is a Support Vector Machine (SVM) a good choice for this classification task? What is the role of the `kernel` parameter (we used `'linear'`)? How might other kernels like `'rbf'` change the decision boundary and potentially the performance?\n",
        "\n",
        "4.  **BoVW Limitations**: The Bag of Visual Words model is powerful, but it has a major limitation: it discards all spatial information about the features. It only counts *how many* of each visual word appear, not *where* they appear. How could this be a problem? For example, could it confuse an image of a bicycle with an image of two separate wheels and handlebars?\n",
        "\n",
        "5.  **Improving the Model**: Based on your results and understanding of the BoVW model, what are two different ways you could try to improve the classifier's accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae88caa5",
      "metadata": {
        "id": "ae88caa5"
      },
      "outputs": [],
      "source": [
        "# The number of clusters for k-Means, which determines the size of the vocabulary.\n",
        "K_CLUSTERS = 50\n",
        "\n",
        "# --- Step 1: SIFT Feature Extraction (for training set) ---\n",
        "print(\"1. Loading training images and extracting SIFT features...\")\n",
        "train_images, train_labels = load_dataset(\"vehicles\", split=\"train\")\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "all_descriptors = []\n",
        "for img in train_images:\n",
        "    pass  # Implement this\n",
        "\n",
        "all_descriptors = np.asarray(all_descriptors)\n",
        "print(\n",
        "    f\"   - Extracted {len(all_descriptors)} total descriptors from {len(train_images)} training images.\"\n",
        ")\n",
        "\n",
        "# --- Step 2: Create a \"Visual Vocabulary\" with k-Means ---\n",
        "\n",
        "# Implement this (use Scikit-Learn's KMeans implementation)\n",
        "kmeans = None\n",
        "\n",
        "\n",
        "# --- Step 3: Represent Images as Feature Histograms ---\n",
        "def create_feature_histograms(images, vocabulary: KMeans):\n",
        "    \"\"\"Creates a histogram of visual words for a list of images.\"\"\"\n",
        "    sift_detector = cv2.SIFT_create()\n",
        "    histograms = []\n",
        "    for image in images:\n",
        "        keypoints, descriptors = sift_detector.detectAndCompute(image, None)\n",
        "\n",
        "        # Create a histogram of size K_CLUSTERS, initialized to zeros\n",
        "        hist = np.zeros(K_CLUSTERS)\n",
        "\n",
        "        if descriptors is not None:\n",
        "            # For each descriptor, find the closest visual word\n",
        "            words = None  # Implement this\n",
        "            # Count the occurrences of each word\n",
        "            word_counts = None  # Implement this (you can use Counter)\n",
        "\n",
        "            # Populate the histogram\n",
        "            pass  # Write the word counts into hist\n",
        "\n",
        "        histograms.append(hist)\n",
        "\n",
        "    return np.asarray(histograms)\n",
        "\n",
        "\n",
        "print(\"\\n3. Creating feature histograms for training and testing sets...\")\n",
        "# Create histograms for the training data\n",
        "X_train = create_feature_histograms(train_images, kmeans)\n",
        "y_train = np.asarray(train_labels)\n",
        "\n",
        "# Create histograms for the testing data\n",
        "test_images, test_labels = load_dataset(\"vehicles\", split=\"test\")\n",
        "X_test = create_feature_histograms(test_images, kmeans)\n",
        "y_test = np.asarray(test_labels)\n",
        "print(\"   - Histograms created.\")\n",
        "\n",
        "# --- Step 4: Train and Evaluate the SVM Classifier ---\n",
        "\n",
        "svm_classifier = None  # Implement this\n",
        "print(\"   - Training complete.\")\n",
        "\n",
        "print(\"\\n--- Evaluating the model ---\")\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the results\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe5c83a1",
      "metadata": {
        "id": "fe5c83a1"
      },
      "source": [
        "# Exercise 4: Color Quantization with k-Means Clustering\n",
        "\n",
        "In this exercise, we will explore an application of clustering outside of feature-based classification. We will use the **k-Means algorithm** for **color quantization**.\n",
        "\n",
        "The goal is to reduce the number of distinct colors in an image to a smaller, representative palette of `k` colors. This is a common technique used in image compression and for creating artistic effects. Instead of classifying images, we will be classifying the *pixels themselves* based on their color.\n",
        "\n",
        "### Your Task:\n",
        "\n",
        "1.  **Load Image**:\n",
        "    * Choose a single, colorful image (e.g., a landscape, a bowl of fruit, a vibrant painting). Make sure the image file is in the same directory as this notebook.\n",
        "    * Load the image using OpenCV. Remember that OpenCV loads images in BGR format. For display purposes, you will want to convert it to RGB.\n",
        "\n",
        "2.  **Prepare Pixel Data**:\n",
        "    * The features for this task are the colors of the individual pixels. To prepare the data for k-Means, you need to reshape the image from a 2D grid of pixels (height x width) into a 1D list of pixels.\n",
        "    * Each pixel is a 3-dimensional data point (its R, G, and B value). Your final data structure should be a NumPy array of shape `(width * height, 3)`.\n",
        "\n",
        "3.  **Apply k-Means Clustering**:\n",
        "    * Use the `sklearn.cluster.KMeans` algorithm on your list of pixel colors.\n",
        "    * Set `k` to the desired number of final colors. Good starting values are `k=8` or `k=16`.\n",
        "    * The algorithm will group all the pixel colors into `k` clusters and find the centroid (the mean color) for each cluster. These `k` centroids will form your new, optimized color palette.\n",
        "\n",
        "4.  **Recreate the Image**:\n",
        "    * Create a new image by replacing each original pixel's color with the color of the centroid it was assigned to by the k-Means algorithm.\n",
        "    * You will need to use the labels assigned by the fitted k-Means model to map each pixel to its new color from the palette (the cluster centers).\n",
        "    * Finally, reshape the list of new pixel colors back to the original image's dimensions (height x width x 3).\n",
        "\n",
        "5.  **Display Results**:\n",
        "    * Display the original image and the new color-quantized image side-by-side to visually compare the results.\n",
        "\n",
        "\n",
        "**Discussion Questions**\n",
        "\n",
        "1.  **The Effect of `k`**: In the code, `K_COLORS` determines the final number of colors. What happens to the final image as you make `k` very small (e.g., 2 or 3)? What happens when you make it larger (e.g., 64)? Discuss the trade-off between the value of `k`, visual fidelity, and the level of compression.\n",
        "\n",
        "2.  **Understanding the Centroids**: What do the `kmeans.cluster_centers_` physically represent in this exercise? Why are these centroids the perfect choice for the new color palette?\n",
        "\n",
        "3.  **Real-World Applications**: Beyond creating artistic effects, where is color quantization useful? Think about old video games or image file formats like GIF. How does this technique help with image compression?\n",
        "\n",
        "4.  **Limitations of k-Means**: Did you notice any strange artifacts or color choices in the quantized image? K-Means only considers color, not the spatial location of pixels. How might this lead to less-than-ideal results on certain images (e.g., an image with a smooth, subtle gradient)?\n",
        "\n",
        "5.  **k-Means Initialization**: The k-Means algorithm's final result can sometimes depend on the initial placement of its clusters. The `n_init` parameter in `sklearn.cluster.KMeans` runs the algorithm multiple times with different starting points and chooses the best result. Why is this important for achieving a good and consistent color palette?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "663460a5",
      "metadata": {
        "id": "663460a5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# --- 1. Load Image ---\n",
        "\n",
        "# Change this filename to the path of your chosen colorful image.\n",
        "IMAGE_PATH = \"images/bowl_of_fruit.jpg\"\n",
        "\n",
        "\n",
        "bgr_image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "# Convert the image from BGR to RGB for correct display with Matplotlib\n",
        "rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "# --- 2. Prepare Pixel Data ---\n",
        "\n",
        "# Get the dimensions of the image\n",
        "height, width, _ = rgb_image.shape\n",
        "\n",
        "# Reshape the image to be a list of pixels (N_pixels, 3)\n",
        "# We also convert to float32, as k-means expects this type.\n",
        "pixel_data = rgb_image.reshape((-1, 3))\n",
        "pixel_data = np.float32(pixel_data)\n",
        "\n",
        "\n",
        "# --- 3. Apply k-Means Clustering ---\n",
        "\n",
        "# Set the number of clusters (desired number of colors)\n",
        "K_COLORS = 10\n",
        "\n",
        "print(f\"\\nApplying k-Means clustering to find {K_COLORS} dominant colors...\")\n",
        "\n",
        "# Create a k-Means model and fit it to the pixel data\n",
        "# n_init='auto' is the default and recommended setting to handle initialization intelligently\n",
        "\n",
        "\n",
        "kmeans = None  # Implement this\n",
        "\n",
        "\n",
        "# The cluster centers are our new color palette.\n",
        "# These are floats, so we convert them back to 8-bit unsigned integers.\n",
        "\n",
        "\n",
        "new_palette = None  # Implement this\n",
        "\n",
        "\n",
        "# --- 4. Recreate the Image ---\n",
        "\n",
        "# `kmeans.labels_` contains the cluster index for each pixel.\n",
        "# We can use this to create the new image.\n",
        "labels = kmeans.labels_\n",
        "quantized_pixels = new_palette[labels]\n",
        "\n",
        "# Reshape the 1D array of pixels back to the original image dimensions\n",
        "quantized_image = quantized_pixels.reshape((height, width, 3))\n",
        "\n",
        "\n",
        "# --- 5. Display Results ---\n",
        "\n",
        "plt.style.use(\"default\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(rgb_image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(quantized_image)\n",
        "plt.title(f\"Color-Quantized Image (k={K_COLORS})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963be2c5",
      "metadata": {
        "id": "963be2c5"
      },
      "source": [
        "## Exercise 5 Simple Object Classification with a Neural Network\n",
        "\n",
        "In this exercise, we will tackle the same problem as before—classifying basic geometric shapes—but this time using **PyTorch**, another major deep learning framework.\n",
        "\n",
        "The goal remains to build, train, and evaluate a neural network to distinguish between \"Circles\" and \"Squares.\" This will give you a chance to see how a different framework handles model definition, training loops, and data handling, which are core skills for any deep learning practitioner.\n",
        "\n",
        "**Tasks**:\n",
        "\n",
        "1.  **Load and Prepare Data**:\n",
        "    * You can use the create shape dataset function that is supplied or implement this yourself.\n",
        "    * The code cell below includes a function to generate the 28x28 grayscale images of circles and squares.\n",
        "    * You will need to flatten each 28x28 image into a 784-element vector and normalize the pixel values to a [0, 1] range.\n",
        "    * Convert the NumPy data arrays into PyTorch Tensors.\n",
        "    * Use PyTorch's `TensorDataset` and `DataLoader` to create efficient data loaders for batching and shuffling.\n",
        "\n",
        "2.  **Define the Network Architecture**:\n",
        "    * Create a simple neural network by defining a class that inherits from PyTorch's `torch.nn.Module`.\n",
        "    * In the `__init__` method, define your layers using `torch.nn.Linear`.\n",
        "        * One hidden layer with a small number of neurons (e.g., 64).\n",
        "        * An output layer with a single neuron.\n",
        "    * In the `forward` method, define the flow of data through the layers, applying a **ReLU** activation to the hidden layer and a **Sigmoid** activation to the output layer.\n",
        "\n",
        "3.  **Define Loss Function and Optimizer**:\n",
        "    * Choose a loss function suitable for binary classification, which is `nn.BCELoss` (Binary Cross-Entropy Loss) in PyTorch.\n",
        "    * Select an optimizer, such as `torch.optim.SGD`, and pass it the model's parameters.\n",
        "\n",
        "4.  **Write the Training Loop**:\n",
        "    * In PyTorch, you write the training loop explicitly.\n",
        "    * For a set number of `epochs`, you will loop through the training `DataLoader`, and for each batch you must:\n",
        "        1.  Perform a forward pass to get predictions.\n",
        "        2.  Compute the loss.\n",
        "        3.  Clear previous gradients (`optimizer.zero_grad()`).\n",
        "        4.  Perform backpropagation (`loss.backward()`).\n",
        "        5.  Update the model's weights (`optimizer.step()`).\n",
        "\n",
        "5. **Tune hyperparameters**\n",
        "    * Try different values for the following hyperparameters:\n",
        "        - Learning rate\n",
        "        - Momentum\n",
        "        - Hidden layer size\n",
        "        - Dropout\n",
        "    * Achieve a validation accuracy of more than 80 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f880c04",
      "metadata": {
        "id": "4f880c04"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Training hyper parameters\n",
        "HIDDEN_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "DROPOUT = 0.0\n",
        "MOMENTUM = 0.0\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "# Data Parameters\n",
        "NUM_SAMPLES_PER_CLASS = 256\n",
        "IMG_SIZE = 28\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "def create_shape_dataset():\n",
        "    def generate_shape_data(num_images, shape_type, img_size=28):\n",
        "        \"\"\"Generates a dataset of simple shapes (circles or squares).\"\"\"\n",
        "        images = []\n",
        "        for _ in range(num_images):\n",
        "            img = np.zeros((img_size, img_size), dtype=np.uint8)\n",
        "            center = (np.random.randint(10, 18), np.random.randint(10, 18))\n",
        "            size = np.random.randint(5, 15)\n",
        "            color = 255\n",
        "\n",
        "            if shape_type == \"circle\":\n",
        "                cv2.circle(img, center, size, color, -1)\n",
        "            elif shape_type == \"square\":\n",
        "                pt1 = (center[0] - size, center[1] - size)\n",
        "                pt2 = (center[0] + size, center[1] + size)\n",
        "                cv2.rectangle(img, pt1, pt2, color, -1)\n",
        "            angle = np.random.randint(-10, 10)\n",
        "            M = cv2.getRotationMatrix2D((center[0], center[1]), angle, 1)\n",
        "            img = cv2.warpAffine(img, M, (img_size, img_size))\n",
        "            noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n",
        "            img = cv2.add(img, noise)\n",
        "            images.append(img)\n",
        "        return np.array(images)\n",
        "\n",
        "    # --- 1. Load and Prepare Data ---\n",
        "    print(\"1. Generating and preparing data for PyTorch...\")\n",
        "\n",
        "    # Generate images\n",
        "    circles = generate_shape_data(NUM_SAMPLES_PER_CLASS, \"circle\", IMG_SIZE)\n",
        "    squares = generate_shape_data(NUM_SAMPLES_PER_CLASS, \"square\", IMG_SIZE)\n",
        "\n",
        "    # Create labels (0 for circle, 1 for square)\n",
        "    circle_labels = np.zeros(circles.shape[0])\n",
        "    square_labels = np.ones(squares.shape[0])\n",
        "\n",
        "    # Combine, flatten, and normalize\n",
        "    X = (\n",
        "        np.concatenate((circles, squares), axis=0)\n",
        "        .reshape(-1, IMG_SIZE * IMG_SIZE)\n",
        "        .astype(\"float32\")\n",
        "        / 255.0\n",
        "    )\n",
        "    y = np.concatenate((circle_labels, square_labels), axis=0).astype(\"float32\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Convert to PyTorch Tensors\n",
        "    X_train_tensor = torch.from_numpy(X_train)\n",
        "    y_train_tensor = torch.from_numpy(y_train).view(-1, 1)  # Reshape for BCELoss\n",
        "    X_test_tensor = torch.from_numpy(X_test)\n",
        "    y_test_tensor = torch.from_numpy(y_test).view(-1, 1)  # Reshape for BCELoss\n",
        "\n",
        "    # Create TensorDatasets and DataLoaders\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
        "    )\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# Change this\n",
        "train_loader, test_loader = None, None\n",
        "\n",
        "print(f\"   - Data loaded into PyTorch DataLoaders with batch size {BATCH_SIZE}.\")\n",
        "\n",
        "# --- 2. Define the Network Architecture ---\n",
        "print(\"\\n2. Defining the Neural Network model using nn.Module...\")\n",
        "\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, hidden_size, dropout=0.0):\n",
        "        pass  # Your code goes here\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass  # Your code goes here\n",
        "\n",
        "\n",
        "model = SimpleNet(hidden_size=HIDDEN_SIZE, dropout=DROPOUT)\n",
        "\n",
        "# --- 3. Define Loss Function and Optimizer ---\n",
        "print(\"\\n3. Defining loss function and optimizer...\")\n",
        "\n",
        "criterion = None  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = None\n",
        "\n",
        "# --- 4. Write the Training Loop ---\n",
        "print(\"\\n4. Training the network...\")\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Implement output and loss calculation\n",
        "        outputs = None\n",
        "        loss = None\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # --- 5. Evaluation within the loop ---\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_val_loss / len(test_loader)\n",
        "    val_acc = correct_val / total_val\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "# --- Final Evaluation ---\n",
        "print(f\"\\nFinal Test Accuracy: {val_accuracies[-1] * 100:.2f}%\")\n",
        "print(f\"Final Test Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.title(\"Model Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc31fc0",
      "metadata": {
        "id": "9cc31fc0"
      },
      "source": [
        "** Discussion Questions**:\n",
        "\n",
        "1. Learning Rate and Convergence: Describe the effect of the learning rate. What did you observe in the training/validation loss curves when you set the learning rate too high (e.g., 0.1) versus too low (e.g., 1e-5)? How does the learning rate control how quickly the model finds an optimal solution?\n",
        "\n",
        "2. Hidden Layer Capacity: Explain the trade-off involved in setting the hidden layer size. What happened to your training and validation accuracy when you used a very small number of neurons (e.g., 8)? What happened with a very large number (e.g., 512)? Relate your findings to the concepts of underfitting (the model is too simple) and overfitting (the model is too complex and memorizes the training data).\n",
        "\n",
        "3. The Role of Dropout: Dropout is a form of regularization. What problem is it designed to solve? Describe where you added the nn.Dropout layer in your model's forward pass. What effect did adding dropout have on the gap between your training accuracy and your validation accuracy?\n",
        "\n",
        "4. The Optimizer and Momentum: The Adam optimizer has built-in adaptive momentum. If you switched to a standard SGD (Stochastic Gradient Descent) optimizer, what was the role of the momentum parameter? How did adding momentum (e.g., a value of 0.9) to SGD affect the training speed and the final accuracy?\n",
        "\n",
        "5. Your Winning Combination: There is no single \"correct\" combination of hyperparameters to achieve the target accuracy. Describe the final set of parameters that worked for you. Which hyperparameter do you feel had the most significant impact on improving the model's performance for this specific task, and why?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3dad",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}