{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajopr/lectures/blob/main/lecture09_deep_learning_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMq_UoRob8km"
      },
      "source": [
        "# Workshop: An Introduction to Deep Learning and Computer Vision\n",
        "\n",
        "Welcome! In this notebook, we will journey from the foundational concepts of deep learning to a hands-on, practical application: fine-tuning a state-of-the-art model for a custom image classification task.\n",
        "\n",
        "**Goals:**\n",
        "1.  **Grasp the Basics:** Understand what Deep Learning and Neural Networks are.\n",
        "2.  **Learn about CNNs:** Discover Convolutional Neural Networks (CNNs), the workhorse of modern computer vision.\n",
        "3.  **Understand Transfer Learning:** Build a strong theoretical foundation for transfer learning, one of the most powerful techniques in AI.\n",
        "4.  **Get Hands-On:** Use modern tools like PyTorch and PyTorch Lightning to train and evaluate a real model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-g-JxFeb8kn"
      },
      "source": [
        "## Part 1: The Theory\n",
        "\n",
        "### What is Deep Learning?\n",
        "\n",
        "Deep Learning is a subfield of machine learning inspired by the structure and function of the human brain. It uses artificial **neural networks** with many layers (hence \"deep\") to learn complex patterns from large amounts of data.\n",
        "\n",
        "A neural network is made of interconnected nodes, or *neurons*, organized in layers. Each neuron receives inputs, performs a simple computation, and passes the result to the next layer. By adjusting the connections between neurons, the network can learn to map inputs (like an image) to outputs (like a label).\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41377-024-01590-3/MediaObjects/41377_2024_1590_Fig3_HTML.png\" alt=\"A simple neural network\">\n",
        "</p>\n",
        "<center>From biology to computation: The image displays the inspiration and architecture of neural networks. (a) A biological neuron. (b) The mathematical model of an artificial neuron. (c) A Multi-Layer Perceptron (MLP), a type of deep neural network, composed of layers of artificial neurons.Neural Network An artificial neural network with an input layer, two hidden layers, and an output layer.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG5JKo9-b8ko"
      },
      "source": [
        "### Convolutional Neural Networks (CNNs) for Vision\n",
        "\n",
        "While a standard neural network can work with images, it's not ideal. It would require an enormous number of parameters and would lose the spatial relationships between pixels (i.e., it wouldn't know that pixels close to each other form a shape).\n",
        "\n",
        "**Convolutional Neural Networks (CNNs)** are a specialized type of neural network designed specifically for visual data. They use three main types of layers to process images efficiently.\n",
        "\n",
        "#### 1. The Convolutional Layer\n",
        "\n",
        "This is the core building block of a CNN. Instead of looking at every pixel individually, the convolutional layer uses **filters** (also called kernels) to scan over the image and detect specific features like edges, corners, colors, and textures. As the filter slides across the image, it produces a **feature map** that highlights where those features are present.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/2D_Convolution_Animation.gif/220px-2D_Convolution_Animation.gif\" alt=\"A convolution operation\">\n",
        "</p>\n",
        "<center>Animation of a 3x3 filter sliding over a 5x5 input to produce a 3x3 feature map.</center>\n",
        "\n",
        "Early layers in the network learn simple features (edges), and deeper layers combine these to learn more complex features (eyes, noses, entire faces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A23lRHx5b8ko"
      },
      "source": [
        "#### 2. The Pooling Layer\n",
        "\n",
        "The pooling layer's job is to reduce the spatial size of the feature maps, which reduces the number of parameters and computational cost in the network. This also helps make the detected features more robust to changes in their position in the image.\n",
        "\n",
        "The most common type is **Max Pooling**, which takes a small window and keeps only the maximum value.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/330px-Max_pooling.png\" alt=\"Max pooling operation\">\n",
        "</p>\n",
        "<center>Max pooling with a 2x2 filter and a stride of 2.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WqtD5_0b8ko"
      },
      "source": [
        "#### 3. The Fully-Connected Layer\n",
        "\n",
        "After several convolutional and pooling layers have extracted features from the image, the high-level features are flattened into a one-dimensional vector. This vector is then fed into a standard neural network (called a fully-connected layer or dense layer) which acts as a classifier. It takes the learned features and decides which class the image belongs to.\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.stack.imgur.com%2FHvPMa.png&f=1&nofb=1&ipt=35b5ab47dabeb3543c1a750b9011c8d5f4de8a85805eea94cdda343b5131787e\" alt=\"A typical CNN architecture\">\n",
        "</p>\n",
        "<center>The architecture of LeNet-5, one of the earliest successful CNNs. It combines convolutional and pooling layers to extract features, followed by fully-connected layers for classification.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwkZ-odxb8ko"
      },
      "source": [
        "### Beyond Classification: Other Common Computer Vision Tasks\n",
        "\n",
        "While we are focusing on classification (assigning a single label to an image), it's useful to know about other powerful tasks that CNNs can perform. These tasks differ in the *modality* or structure of their output.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. Image Classification & Localization\n",
        "* **Question:** What is in the image, and roughly where is it?\n",
        "* **Output:** A class label and a single **bounding box** that outlines the main object.\n",
        "* **Example:** \"This image contains a 'cat' at these coordinates [x, y, width, height].\"\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:304/1*uI4AaqoDew9p9YRsVFDZNg.png\" alt=\"Classification and Localization\">\n",
        "    <br>\n",
        "    <em>Classification and Localization</em>\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Object Detection\n",
        "* **Question:** What objects are in the image, and where are they?\n",
        "* **Output:** Multiple **bounding boxes**, each with its own class label. This is a step up from localization as it can handle multiple objects.\n",
        "* **Example:** \"Found a 'dog' at [box 1], a 'bicycle' at [box 2] and a 'truck' [box 3].\"\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://viso.ai/wp-content/uploads/2021/02/yolo-object-detection.jpg\" alt=\"Object Detection\">\n",
        "    <br>\n",
        "    <em>Object detection</em>\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Semantic Segmentation\n",
        "* **Question:** What is the exact outline of each *category* of object in the image?\n",
        "* **Output:** A pixel-level mask. Every pixel in the image is assigned a class label (e.g., 'car', 'road', 'sky', 'person'). It does not distinguish between different instances of the same class.\n",
        "* **Example:** \"All these pixels belong to 'dog', these pixels belong to 'grass', these pixels to 'wall'.\"\n",
        "\n",
        "#### 4. Instance Segmentation\n",
        "* **Question:** What is the exact outline of each *individual object* in the image?\n",
        "* **Output:** A pixel-level mask for each distinct object instance. This is a combination of object detection and semantic segmentation.\n",
        "* **Example:** \"This is 'dog 1', this is 'dog 2'\"\n",
        "\n",
        "#### 5. Panoptic Segmentation\n",
        "* **Question:** What is the outline of every object and background region in the image, combining both semantic and instance segmentation?\n",
        "* **Output:** Each pixel is assigned both a class label and, for countable objects, an instance ID. Uncountable regions (like sky, road) are labeled as \"stuff,\" while countable objects (like people, cars) are labeled as \"things\" with unique IDs.\n",
        "* **Example:** \"These pixels are 'dog 1', these pixels are 'dog 2', these are 'grass' and these are 'wall'\"\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://images.prismic.io/encord/89dcc49f-2ce2-4b93-bbcb-7d0c96f2ebda_Instance+Segmentation+-+Encord.png\" alt=\"Instance Segmentation\">\n",
        "    <br>\n",
        "</p>\n",
        "<center>\n",
        "    <em>\n",
        "        a) Original image &nbsp;&nbsp; b) Semantic segmentation &nbsp;&nbsp; c) Instance segmentation &nbsp;&nbsp; d) Panoptic segmentation\n",
        "    </em>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8qM9x9Zb8kp"
      },
      "source": [
        "## Part 2: Hands-On Transfer Learning\n",
        "\n",
        "Now that we have a theoretical base, let's put it into practice. We will fine-tune a pre-trained CNN to classify different types of vehicles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yylqz4pob8kp"
      },
      "source": [
        "### 1. Setup - Installing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRTFxV_tb8kq"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q pytorch-lightning timm matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctQcihOBb8kr"
      },
      "source": [
        "### 2. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUX6uLIpb8kr"
      },
      "source": [
        "To follow along with this workshop, please add the following Google Drive folder to your own Google Drive:\n",
        "\n",
        "**[Vehicle Images Dataset - Google Drive Link](https://drive.google.com/drive/folders/16aqOtiz56fG_F02aD-95P_68YnHsdngn?usp=sharing)**\n",
        "\n",
        "1. Click the link above.\n",
        "2. Click the \"Add shortcut to Drive\" button at the top.\n",
        "3. Choose a location in your Drive and confirm.\n",
        "\n",
        "This will make the dataset accessible for mounting and use in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WufRoBNQb8kr"
      },
      "outputs": [],
      "source": [
        "# mount google drive folder\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Copy images to local folder for faster access\n",
        "shutil.copytree(\"/content/drive/MyDrive/vehicles\", \"/content\", dirs_exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qLjW1qqb8kr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Download and extract the dataset\n",
        "DATA_DIR = \"C:/users/danielproell/Downloads/vehicles\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR37vsNQb8ks"
      },
      "source": [
        "#### The PyTorch Lightning DataModule\n",
        "\n",
        "A Lightning DataModule is a standardized way to organize and encapsulate all the code related to data processing in PyTorch Lightning. It separates the data processing logic from the model code, making your project more organized and maintainable.\n",
        "\n",
        "#### Core Components of a DataModule\n",
        "\n",
        "##### 1. Data Preparation and Loading\n",
        "- **prepare_data()**: Downloads data, processes data, etc. Called only on one GPU in distributed settings.\n",
        "- **setup()**: Creates datasets, splits data, etc. Called on every GPU.\n",
        "- **DataLoaders**: Methods that return PyTorch DataLoaders for training, validation, and testing.\n",
        "\n",
        "##### 2. Transforms\n",
        "Transforms are essential for several reasons:\n",
        "- **Data Augmentation**: Increases the diversity of your training data by applying random transformations (rotations, flips, color jittering) to help your model generalize better.\n",
        "- **Normalization**: Standardizes your input data to have similar statistical properties, which helps neural networks converge faster during training.\n",
        "- **Preprocessing**: Converts raw data into a format suitable for your model (resizing images, converting to tensors, etc.).\n",
        "\n",
        "##### 3. Dataset Organization\n",
        "- Defines how raw data is converted into PyTorch Datasets\n",
        "- Handles data splitting (train/val/test)\n",
        "- Manages any data sampling strategies\n",
        "\n",
        "#### Benefits of Using a DataModule\n",
        "\n",
        "1. **Reproducibility**: Encapsulates all randomization and processing steps.\n",
        "2. **Portability**: Makes it easy to share and reuse data pipelines across projects.\n",
        "3. **Code Organization**: Separates data concerns from model architecture.\n",
        "4. **Distributed Training Support**: Works seamlessly in multi-GPU environments.\n",
        "\n",
        "#### Example Structure\n",
        "\n",
        "```python\n",
        "class MyDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size, transforms=None):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def prepare_data(self):\n",
        "        # Download data, process data, etc.\n",
        "        # Only called on one GPU\n",
        "        \n",
        "    def setup(self, stage=None):\n",
        "        # Create datasets, split data, etc.\n",
        "        # Called on every GPU\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "        \n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
        "        \n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M96Z3LnXb8ks"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import pytorch_lightning as pl\n",
        "import os\n",
        "\n",
        "\n",
        "class VehicleDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size=32, num_workers=2, img_size=224):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.img_size = img_size\n",
        "        self.save_hyperparameters()\n",
        "        self.persistent_workers = os.name == \"nt\"\n",
        "\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.train_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = ImageFolder(\n",
        "            self.data_dir + \"/train\", transform=self.train_transform\n",
        "        )\n",
        "\n",
        "        self.val_dataset = ImageFolder(self.data_dir + \"/val\", transform=self.transform)\n",
        "        self.class_names = self.train_dataset.classes\n",
        "        print(f\"Found {len(self.class_names)} classes: {self.class_names}\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            persistent_workers=self.persistent_workers,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            persistent_workers=self.persistent_workers,\n",
        "        )\n",
        "\n",
        "\n",
        "data_module = VehicleDataModule(data_dir=DATA_DIR)\n",
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFdQrI4wb8ks"
      },
      "source": [
        "### 3. The Theory of Transfer Learning\n",
        "\n",
        "**What is it?**\n",
        "\n",
        "Transfer learning is a technique where a model trained on a large, general task (e.g., classifying 1000 types of objects in ImageNet) is reused as the starting point for a different, more specific task (e.g., classifying our vehicle types).\n",
        "\n",
        "**Why use it?**\n",
        "\n",
        "1.  **Less Data Needed:** You leverage the 'knowledge' the model has already gained, so you don't need a massive dataset for your specific problem.\n",
        "2.  **Faster Training:** Training converges much faster because the model's weights are already a great starting point.\n",
        "3.  **Better Performance:** Pre-trained models are often highly optimized and learn very effective general features.\n",
        "\n",
        "We do this by taking the pre-trained network, freezing the early layers that detect general features, and only training (or *fine-tuning*) the final layers to specialize them for our task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1C6EBNJb8ks"
      },
      "source": [
        "#### The PyTorch Lightning Module\n",
        "\n",
        "The `LightningModule` organizes our model, training, and validation logic in one clean class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX-v7cA1b8kt"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "\n",
        "\n",
        "class VehicleClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = timm.create_model(\n",
        "            \"efficientnetv2_rw_s.ra2_in1k\", pretrained=True, num_classes=num_classes\n",
        "        )\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "        metrics = torchmetrics.MetricCollection(\n",
        "            {\n",
        "                \"accuracy\": torchmetrics.classification.MulticlassAccuracy(\n",
        "                    num_classes=num_classes\n",
        "                ),\n",
        "                \"precision\": torchmetrics.classification.MulticlassPrecision(\n",
        "                    num_classes=num_classes, average=\"macro\"\n",
        "                ),\n",
        "                \"recall\": torchmetrics.classification.MulticlassRecall(\n",
        "                    num_classes=num_classes, average=\"macro\"\n",
        "                ),\n",
        "                \"f1\": torchmetrics.classification.MulticlassF1Score(\n",
        "                    num_classes=num_classes, average=\"macro\"\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "        self.train_metrics = metrics.clone(prefix=\"train_\")\n",
        "        self.val_metrics = metrics.clone(prefix=\"val_\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        predictions = self.forward(images)\n",
        "        loss = self.loss_function(predictions, labels)\n",
        "        self.train_metrics.update(predictions, labels)\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        predictions = self.forward(images)\n",
        "        loss = self.loss_function(predictions, labels)\n",
        "        self.val_metrics.update(predictions, labels)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        train_metrics = self.train_metrics.compute()\n",
        "        self.log_dict(train_metrics, prog_bar=True)\n",
        "        self.train_metrics.reset()\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_metrics = self.val_metrics.compute()\n",
        "        self.log_dict(val_metrics, prog_bar=True)\n",
        "        self.val_metrics.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "model = VehicleClassifier(num_classes=len(data_module.class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQo0tmFlb8kt"
      },
      "source": [
        "### 4. Train the Model\n",
        "\n",
        "**Tasks**\n",
        "1. Configure a new Trainer. In addition to setting max_epochs=5, add the parameter limit_train_batches=3. This tells the trainer to only use the first 3 batches of data for training in each epoch.\n",
        "2. Train your model for 5 epochs using all the available training data. This will give us a standard performance metric to which we can compare our other experiments.\n",
        "    - Ensure that you instantiate a new model (with a new name) and trainer so that the training begins from scratch\n",
        "3. Compare the resulting models using the inference code and the tensorboard dashboard below\n",
        "4. Go to torchvision's homepage and select a transform to add to the training process that you think might make the model learn to generalize to unseen images better and add it to the train transforms in the datamodule above. Train a new model with limited data and see if the model achieved better results than without the transform\n",
        "    - https://docs.pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrJnUR1kb8kt"
      },
      "outputs": [],
      "source": [
        "model = VehicleClassifier(num_classes=len(data_module.class_names))\n",
        "trainer = pl.Trainer()\n",
        "\n",
        "print(\"Starting model fine-tuning...\")\n",
        "trainer.fit(model, data_module)\n",
        "print(\"Training finished!\")\n",
        "\n",
        "print(f\"Best model path: {trainer.checkpoint_callback.best_model_path}\")\n",
        "\n",
        "best_model = VehicleClassifier.load_from_checkpoint(\n",
        "    trainer.checkpoint_callback.best_model_path\n",
        ")\n",
        "best_model.eval()\n",
        "best_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW_e-e7Wb8kt"
      },
      "source": [
        "### 5. Making Predictions (Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_c5Hy2Cb8kt"
      },
      "outputs": [],
      "source": [
        "val_loader = data_module.val_dataloader()\n",
        "images, labels = next(iter(val_loader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = best_model(images.to(best_model.device))\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "class_names = data_module.class_names\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
        "    std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(16):\n",
        "    if i >= len(images):\n",
        "        break\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    img = inv_normalize(images[i])\n",
        "    img = img.permute(1, 2, 0)\n",
        "\n",
        "    predicted_class = class_names[preds[i]]\n",
        "    true_class = class_names[labels[i]]\n",
        "    title_color = \"g\" if predicted_class == true_class else \"r\"\n",
        "\n",
        "    plt.imshow(img.cpu().numpy())\n",
        "    plt.title(f\"True: {true_class}\\nPred: {predicted_class}\", color=title_color)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCU5iDZEb8ku"
      },
      "source": [
        "### 5a. Training dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlfYz3fwb8ku"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zi4ahPsb8ku"
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "Congratulations! You have successfully walked through a complete, end-to-end deep learning pipeline. You have:\n",
        "\n",
        "- Learned the **theory** behind Deep Learning and CNNs.\n",
        "- Understood the power and process of **Transfer Learning**.\n",
        "- Organized a data pipeline using a **PyTorch Lightning DataModule**.\n",
        "- Built and trained a model using a **LightningModule**, leveraging a state-of-the-art **EfficientNetV2** architecture.\n",
        "- Used the trained model to **make predictions** on unseen data and visualize the results.\n",
        "\n",
        "This workshop demonstrates a fundamental and highly effective workflow in modern computer vision. The skills you've learned here are directly applicable to a wide range of real-world problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lptr9lh5b8ku"
      },
      "source": [
        "# Workshop: Exploring Computer Vision Tasks with Hugging Face ðŸ¤—\n",
        "\n",
        "Welcome! In the previous workshop, we saw how to fine-tune a model for a specific classification task. But what if we want to perform other tasks, like finding multiple objects in an image or editing a picture with text commands, without having to train a new model from scratch?\n",
        "\n",
        "This is where the Hugging Face Hub and its `pipeline` API come in. We can use powerful, pre-trained models for a huge variety of tasks with just a few lines of code.\n",
        "\n",
        "**Goals:**\n",
        "1.  **Understand Zero-Shot Learning:** See how a model can perform tasks it wasn't explicitly trained for, like finding specific parts of a car.\n",
        "2.  **Perform Object Detection:** Use a zero-shot model to identify and locate different parts of a vehicle in an image.\n",
        "3.  **Perform Image Segmentation:** Learn the difference between bounding boxes and pixel-level masks by using a model to segment an image.\n",
        "4.  **Perform Image-to-Image Transformation:** Have some fun by editing images based on text instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oivz3eWdb8ku"
      },
      "source": [
        "## 1. Setup - Installing Necessary Libraries\n",
        "\n",
        "First, we need to install the libraries. `transformers` is the core Hugging Face library, `timm` is needed for some vision model backbones, and `Pillow` is used for image manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jDqyE1Vb8kv"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch timm Pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PfzeTrfb8kv"
      },
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Let's use the same vehicle dataset from our last session. This will give us a consistent set of images to work with. We'll download it and then pick a few sample images to use in our exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5UPCGfAb8kv"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from PIL import Image, ImageDraw\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "\n",
        "try:\n",
        "    DATA_DIR = \"vehicles\"\n",
        "    train_dir = os.path.join(DATA_DIR, \"train\")\n",
        "    classes = list(os.listdir(train_dir))\n",
        "    image_files = glob.glob(os.path.join(train_dir, \"**\", \"*.jpg\"), recursive=True)\n",
        "    if not image_files:\n",
        "        raise FileNotFoundError\n",
        "    SAMPLE_IMAGE_PATH = random.choice(image_files)\n",
        "    print(f\"Using sample image: {SAMPLE_IMAGE_PATH}\")\n",
        "except FileNotFoundError:\n",
        "    print(\n",
        "        \"Could not find sample images. Please check the dataset path and check drive mounting above.\"\n",
        "    )\n",
        "    SAMPLE_IMAGE_PATH = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xx7oFH0b8kv"
      },
      "source": [
        "## Task 1: Zero-Shot Object Detection\n",
        "\n",
        "**Concept:** Standard object detectors are trained on a fixed set of classes (e.g., the 80 classes in the COCO dataset). **Zero-shot** detectors are different. They use a vision-language model (like CLIP) that understands the relationship between images and text. This allows us to provide arbitrary text labels at inference time, and the model can find objects matching those descriptions, even if it never saw that specific label during training!\n",
        "\n",
        "Let's use this to find not just cars, but specific *parts* of a vehicle.\n",
        "\n",
        "**Your tasks**\n",
        "1. Change the labels to be found to include some common car parts in the code below\n",
        "2. Try the detector on multiple image and look at the predictions\n",
        "    - Are detections missing? Are things being wrongly detected?\n",
        "3. Change the score being accepted to a reasonable value\n",
        "4. (Optional) Try out the prediction pipeline on another task using your own images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ksCTSyb8kv"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the zero-shot object detection pipeline\n",
        "print(\"Loading zero-shot object detection pipeline...\")\n",
        "detector = pipeline(\n",
        "    model=\"google/owlvit-base-patch32\", task=\"zero-shot-object-detection\"\n",
        ")\n",
        "print(\"Pipeline loaded!\")\n",
        "\n",
        "# Open our sample image\n",
        "if SAMPLE_IMAGE_PATH:\n",
        "    image = Image.open(SAMPLE_IMAGE_PATH)\n",
        "    image_draw = image.copy()\n",
        "    # Define candidate labels. The model will search for these in the image.\n",
        "    candidate_labels = [\"car\"]\n",
        "\n",
        "    # Run the detection\n",
        "    predictions = detector(image, candidate_labels=candidate_labels)\n",
        "\n",
        "    # --- Visualize the results ---\n",
        "    draw = ImageDraw.Draw(image_draw)\n",
        "\n",
        "    # Create a color map for labels\n",
        "    label_colors = {\n",
        "        label: (\n",
        "            random.randint(60, 255),\n",
        "            random.randint(60, 255),\n",
        "            random.randint(60, 255),\n",
        "        )\n",
        "        for label in candidate_labels\n",
        "    }\n",
        "\n",
        "    for prediction in predictions:\n",
        "        box = prediction[\"box\"]\n",
        "        label = prediction[\"label\"]\n",
        "        score = prediction[\"score\"]\n",
        "\n",
        "        if score > 0.15:  # Only draw boxes with a reasonable confidence score\n",
        "            xmin, ymin, xmax, ymax = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
        "            color = label_colors[label]\n",
        "\n",
        "            # Draw the bounding box\n",
        "            draw.rectangle((xmin, ymin, xmax, ymax), outline=color, width=3)\n",
        "\n",
        "            # Draw the label and score\n",
        "            text = f\"{label}: {score:.2f}\"\n",
        "            # We don't have a font file in this environment, so we'll use the default\n",
        "            draw.text((xmin, ymin - 10), text, fill=color)\n",
        "\n",
        "    print(\"Displaying detection results:\")\n",
        "    display(image_draw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lDxCoU0b8kv"
      },
      "source": [
        "## Task 2: Image Segmentation\n",
        "\n",
        "**Concept:** While object detection draws a box *around* an object, segmentation goes a step further. It classifies every single **pixel** in the image. This gives us a precise, pixel-perfect outline of each object.\n",
        "\n",
        "We will use a **panoptic segmentation** model, which is a hybrid: it finds individual instances of objects (like object detection) and also classifies background stuff (like semantic segmentation).\n",
        "\n",
        "**Your tasks**\n",
        "1. Select a model to use for semantic segmentation\n",
        "    - Go to huggingface's models (https://huggingface.co/models) page to search for models\n",
        "    - You can use Facebook's DETR panoptic model with Resnet50 backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qND1OPdb8kw"
      },
      "outputs": [],
      "source": [
        "# Load the image segmentation pipeline\n",
        "print(\"Loading image segmentation pipeline...\")\n",
        "\n",
        "model_name = \"model_name\"\n",
        "\n",
        "segmenter = pipeline(model=model_name, task=\"image-segmentation\")\n",
        "print(\"Pipeline loaded!\")\n",
        "\n",
        "if SAMPLE_IMAGE_PATH:\n",
        "    image = Image.open(SAMPLE_IMAGE_PATH)\n",
        "    predictions = segmenter(image)\n",
        "\n",
        "    # --- Visualize the results ---\n",
        "    # The 'draw_panoptic_segmentation' function is a utility provided by the model's feature extractor\n",
        "    # Since we're using the pipeline, we'll recreate a simplified version for visualization.\n",
        "\n",
        "    def draw_segmentation(original_image, segmentation_results):\n",
        "        # Create a blank RGBA image to draw the colored masks on\n",
        "        mask_image = Image.new(\"RGBA\", original_image.size, (0, 0, 0, 0))\n",
        "        draw = ImageDraw.Draw(mask_image)\n",
        "\n",
        "        for segment in segmentation_results:\n",
        "            mask = segment[\"mask\"]\n",
        "            label = segment[\"label\"]\n",
        "\n",
        "            # Generate a random color for this segment's label\n",
        "            color = (\n",
        "                random.randint(60, 255),\n",
        "                random.randint(60, 255),\n",
        "                random.randint(60, 255),\n",
        "                150,\n",
        "            )  # RGBA with transparency\n",
        "\n",
        "            # The mask is a binary image. We can use it to color our overlay.\n",
        "            draw.bitmap((0, 0), mask, fill=color)\n",
        "\n",
        "        # Composite the original image with the colored masks\n",
        "        return Image.alpha_composite(original_image.convert(\"RGBA\"), mask_image)\n",
        "\n",
        "    segmented_image = draw_segmentation(image, predictions)\n",
        "    print(\"Displaying segmentation results:\")\n",
        "    display(segmented_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZGTUopKb8kw"
      },
      "source": [
        "## Task 3: Image Enhancement with Super-Resolution\n",
        "\n",
        "**Concept:** Super-resolution is a technique that increases the resolution of an image while preserving and enhancing details. We'll use a pre-trained super-resolution model to upscale one of our lower-resolution images.\n",
        "\n",
        "1. Select a smaller image from your dataset\n",
        "2. Use the existing Swin2SR pipeline to upscale the image\n",
        "3. Compare the original and upscaled versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COMJkk7Ob8kw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image-to-image pipeline\n",
        "# Note: This model is larger and may take more time to download.\n",
        "# We also specify torch_dtype=torch.float16 for memory efficiency if a GPU is available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img2img = pipeline(\n",
        "    \"image-to-image\",\n",
        "    model=\"caidas/swin2SR-lightweight-x2-64\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "image = skimage.io.imread(\n",
        "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse2.explicit.bing.net%2Fth%2Fid%2FOIP.gw2_0gNMn7S2TMe38z5aRwAAAA%3Fr%3D0%26pid%3DApi&f=1&ipt=3f1ca19f9f3d3ba31ad4ca4cb8b0ab575eccba80dade207783e49d555d673628&ipo=images\"\n",
        ")\n",
        "image = Image.fromarray(image)\n",
        "if SAMPLE_IMAGE_PATH:\n",
        "    # Run the transformation\n",
        "    transformed_image = image\n",
        "\n",
        "    # --- Visualize the results ---\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    ax1.imshow(image)\n",
        "    ax1.set_title(\"Original Image\")\n",
        "    ax1.axis(\"off\")\n",
        "\n",
        "    ax2.imshow(transformed_image)\n",
        "    ax2.set_title(\"Transformed: 'Upscaled'\")\n",
        "    ax2.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4iaQJIhb8kx"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this workshop, we barely scratched the surface of what's possible with pre-trained models from the Hugging Face Hub. We saw how to:\n",
        "\n",
        "- **Detect custom objects** with a zero-shot detector.\n",
        "- **Create pixel-perfect masks** with a segmentation model.\n",
        "- **Edit images like a pro** using a simple text command.\n",
        "\n",
        "The key takeaway is that you don't always need to train a model from scratch. By leveraging the work of the broader community, you can build powerful and exciting computer vision applications quickly and effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYUy55X1b8kx"
      },
      "source": [
        "# Optional exercise â€“ Fire and Smoke Detection\n",
        "\n",
        "In this optional exercise, you will use a pre-trained model from the Hugging Face Hub to perform a real-world task: detecting fire and smoke in a dataset of images. This demonstrates the power of transfer learning and using existing models for new applications without having to train from scratch.\n",
        "\n",
        "- Use some of the images found in this drive:\n",
        "    https://drive.google.com/drive/folders/1V61wHR-CjFRaV2RXJfDw0ZzJ5zQJFvpW?usp=sharing\n",
        "- Evaluate the performance of the object detection model\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3dad",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}